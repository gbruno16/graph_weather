{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f5de76-bd5a-4f59-8df8-d0e00cf2697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'graph_weather'...\n",
      "remote: Enumerating objects: 1113, done.\u001b[K\n",
      "remote: Counting objects: 100% (667/667), done.\u001b[K\n",
      "remote: Compressing objects: 100% (308/308), done.\u001b[K\n",
      "remote: Total 1113 (delta 448), reused 467 (delta 327), pack-reused 446\u001b[K\n",
      "Receiving objects: 100% (1113/1113), 4.45 MiB | 10.02 MiB/s, done.\n",
      "Resolving deltas: 100% (655/655), done.\n",
      "/graph_weather_repo\n",
      "Branch 'dgl' set up to track remote branch 'dgl' from 'origin'.\n",
      "Switched to a new branch 'dgl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/gbruno16/graph_weather.git\n",
    "# Rename folder\n",
    "!mv graph_weather graph_weather_repo\n",
    "%cd graph_weather_repo\n",
    "\n",
    "!git checkout dgl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4c0018-d234-4b98-86e4-8ecf1a221889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.2.0\n",
      "Uninstalling torch-2.2.0:\n",
      "  Successfully uninstalled torch-2.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.3.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.18.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.3.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.3.0 (from torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0) (10.0.1)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.0\n",
      "    Uninstalling torchvision-0.17.0:\n",
      "      Successfully uninstalled torchvision-0.17.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.0\n",
      "    Uninstalling torchaudio-2.2.0:\n",
      "      Successfully uninstalled torchaudio-2.2.0\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch -y\n",
    "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8483c839-b973-41ea-bc0f-43842b8361c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n",
      "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_scatter-2.1.2%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2+pt23cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_sparse-0.6.18%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from torch-sparse)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-sparse) (1.26.3)\n",
      "Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, torch-sparse\n",
      "Successfully installed scipy-1.13.1 torch-sparse-0.6.18+pt23cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_cluster-1.6.3%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-cluster) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-cluster) (1.26.3)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.3+pt23cu121\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
      "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-_8g88nwe\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-_8g88nwe\n",
      "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 2be110e4594e3b599688dd5ffe1f8214d4322696\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiohttp (from torch-geometric==2.6.0)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (2023.12.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (1.26.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (5.9.0)\n",
      "Collecting pyparsing (from torch-geometric==2.6.0)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (2.31.0)\n",
      "Collecting scikit-learn (from torch-geometric==2.6.0)\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric==2.6.0) (4.65.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric==2.6.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric==2.6.0) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric==2.6.0)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric==2.6.0)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric==2.6.0)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric==2.6.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric==2.6.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.6.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric==2.6.0) (2023.11.17)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->torch-geometric==2.6.0)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->torch-geometric==2.6.0)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.6.0-py3-none-any.whl size=1118251 sha256=dee7bb5a4f14b69c0e9c582cf0b3aa984d88fc985e3c1ca7f61592d72f9da3ef\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mvwur1s3/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: threadpoolctl, pyparsing, multidict, joblib, frozenlist, async-timeout, yarl, scikit-learn, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 joblib-1.4.2 multidict-6.0.5 pyparsing-3.1.2 scikit-learn-1.5.0 threadpoolctl-3.5.0 torch-geometric-2.6.0 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyshtools\n",
      "  Downloading pyshtools-4.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pyshtools) (1.26.3)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from pyshtools) (1.13.1)\n",
      "Collecting matplotlib>=3.3 (from pyshtools)\n",
      "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting astropy>=4.0 (from pyshtools)\n",
      "  Downloading astropy-6.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting xarray (from pyshtools)\n",
      "  Downloading xarray-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pyshtools) (2.31.0)\n",
      "Collecting pooch>=1.1 (from pyshtools)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pyshtools) (4.65.0)\n",
      "Collecting pyerfa>=2.0.1.1 (from astropy>=4.0->pyshtools)\n",
      "  Downloading pyerfa-2.0.1.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting astropy-iers-data>=0.2024.5.27.0.30.8 (from astropy>=4.0->pyshtools)\n",
      "  Downloading astropy_iers_data-0.2024.6.17.0.31.35-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: PyYAML>=3.13 in /opt/conda/lib/python3.10/site-packages (from astropy>=4.0->pyshtools) (6.0.1)\n",
      "Requirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from astropy>=4.0->pyshtools) (23.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3->pyshtools)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3->pyshtools)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3->pyshtools)\n",
      "  Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.3->pyshtools)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->pyshtools) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pyshtools) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pyshtools) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pyshtools) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pyshtools) (2023.11.17)\n",
      "Collecting pandas>=2.0 (from xarray->pyshtools)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0->xarray->pyshtools) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0->xarray->pyshtools)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->pyshtools) (1.16.0)\n",
      "Downloading pyshtools-4.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astropy-6.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2024.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astropy_iers_data-0.2024.6.17.0.31.35-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyerfa-2.0.1.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.7/738.7 kB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pyerfa, kiwisolver, fonttools, cycler, contourpy, astropy-iers-data, pooch, pandas, matplotlib, astropy, xarray, pyshtools\n",
      "Successfully installed astropy-6.1.1 astropy-iers-data-0.2024.6.17.0.31.35 contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.0 kiwisolver-1.4.5 matplotlib-3.9.0 pandas-2.2.2 pooch-1.8.2 pyerfa-2.0.1.4 pyshtools-4.12.2 tzdata-2024.1 xarray-2024.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.4 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest) (2.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/google-research/weatherbench2.git\n",
      "  Cloning https://github.com/google-research/weatherbench2.git to /tmp/pip-req-build-0sh5_8y3\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/weatherbench2.git /tmp/pip-req-build-0sh5_8y3\n",
      "  Resolved https://github.com/google-research/weatherbench2.git to commit 16e0131309a2b3916875a2cf8806190ffedea308\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting apache-beam>=2.31.0 (from weatherbench2==0.2.0)\n",
      "  Downloading apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting jax[cpu] (from weatherbench2==0.2.0)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from weatherbench2==0.2.0) (1.26.3)\n",
      "Collecting pandas==2.0.3 (from weatherbench2==0.2.0)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from weatherbench2==0.2.0) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from weatherbench2==0.2.0) (1.5.0)\n",
      "Collecting xarray==2023.7.0 (from weatherbench2==0.2.0)\n",
      "  Downloading xarray-2023.7.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting xarray-beam (from weatherbench2==0.2.0)\n",
      "  Downloading xarray_beam-0.6.3-py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting zarr (from weatherbench2==0.2.0)\n",
      "  Downloading zarr-2.18.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3->weatherbench2==0.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3->weatherbench2==0.2.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas==2.0.3->weatherbench2==0.2.0) (2024.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from xarray==2023.7.0->weatherbench2==0.2.0) (23.1)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting orjson<4,>=3.9.7 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle~=2.2.1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<2,>=1.33.1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httplib2<0.23.0,>=0.8 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam>=2.31.0->weatherbench2==0.2.0) (4.19.2)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading jsonpickle-3.2.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading pymongo-4.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading redis-5.0.6-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting regex>=2020.6.8 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam>=2.31.0->weatherbench2==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam>=2.31.0->weatherbench2==0.2.0) (4.9.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam>=2.31.0->weatherbench2==0.2.0) (0.19.0)\n",
      "Collecting pyarrow<15.0.0,>=3.0.0 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting js2py<1,>=0.74 (from apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax[cpu]->weatherbench2==0.2.0)\n",
      "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax[cpu]->weatherbench2==0.2.0)\n",
      "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from jax[cpu]->weatherbench2==0.2.0)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->weatherbench2==0.2.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->weatherbench2==0.2.0) (3.5.0)\n",
      "Collecting dask (from xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading dask-2024.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting immutabledict (from xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rechunker>=0.5.1 (from xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading rechunker-0.5.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting asciitree (from zarr->weatherbench2==0.2.0)\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numcodecs>=0.10.0 (from zarr->weatherbench2==0.2.0)\n",
      "  Downloading numcodecs-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam>=2.31.0->weatherbench2==0.2.0) (3.1.2)\n",
      "Collecting tzlocal>=1.2 (from js2py<1,>=0.74->apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam>=2.31.0->weatherbench2==0.2.0)\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (0.10.6)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from pymongo<5.0.0,>=3.8.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (2.5.0)\n",
      "Collecting mypy-extensions (from rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from redis<6,>=5.0.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache-beam>=2.31.0->weatherbench2==0.2.0) (2023.11.17)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask->xarray-beam->weatherbench2==0.2.0) (8.1.7)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask->xarray-beam->weatherbench2==0.2.0) (2023.12.2)\n",
      "Collecting partd>=1.2.0 (from dask->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask->xarray-beam->weatherbench2==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask->xarray-beam->weatherbench2==0.2.0) (0.12.0)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.13.0->dask->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting locket (from partd>=1.2.0->dask->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting bokeh>=2.4.2 (from dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading bokeh-3.4.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /opt/conda/lib/python3.10/site-packages (from dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.2 in /opt/conda/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0) (1.2.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0) (10.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/conda/lib/python3.10/site-packages (from bokeh>=2.4.2->dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0) (6.4)\n",
      "Collecting xyzservices>=2021.09.1 (from bokeh>=2.4.2->dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0)\n",
      "  Downloading xyzservices-2024.6.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.10.3->dask[array,diagnostics]->rechunker>=0.5.1->xarray-beam->weatherbench2==0.2.0) (2.1.3)\n",
      "Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2023.7.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xarray_beam-0.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zarr-2.18.2-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.2/210.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-3.2.1-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numcodecs-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)\n",
      "Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Downloading pymongo-4.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (669 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.1/669.1 kB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rechunker-0.5.2-py3-none-any.whl (22 kB)\n",
      "Downloading redis-5.0.6-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask-2024.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading bokeh-3.4.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading xyzservices-2024.6.0-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: weatherbench2, crcmod, dill, hdfs, asciitree, pyjsparser, docopt\n",
      "  Building wheel for weatherbench2 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for weatherbench2: filename=weatherbench2-0.2.0-py3-none-any.whl size=73614 sha256=6dc4c84b76f2379998fe1af434d971f5f52d9d4f2c8fea606ddd50d612b4c3a8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ccc0l9ku/wheels/c1/70/d4/6e5cc5122e6d712033eaad6e0607efbacc92fd6ec94f42a6ef\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=23514 sha256=71ffa11ca30cc7b7ced715505eba0f273c48ed77e6f76efcb909c41f2538505d\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=487d932826008a1ec5706164961f15724b329a56517d087af42cc29feff93509\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=3530145cd236134e27159bf29de2d0afb874b456c8388c5b3cbc6e2349fd8402\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=7aec35ee47f35976e44f1679723e8ff6c120bc5efe27a24eb2eb007bf33ac00d\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=a433622ed3b2e67f11b4b30f616d85eb187f65e593491b317d8d805fbb69a7c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=ab8b9d97016a1eae7c96fd79f480b1d46ed7d04ff6c19ce4a44bd7f742eafe04\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built weatherbench2 crcmod dill hdfs asciitree pyjsparser docopt\n",
      "Installing collected packages: pyjsparser, docopt, crcmod, asciitree, zipp, xyzservices, tzlocal, regex, redis, pymongo, pydot, pyarrow-hotfix, pyarrow, protobuf, orjson, opt-einsum, objsize, numcodecs, mypy-extensions, ml-dtypes, locket, jsonpickle, immutabledict, httplib2, grpcio, fasteners, fastavro, dill, cloudpickle, zarr, proto-plus, partd, pandas, js2py, jaxlib, importlib-metadata, hdfs, xarray, jax, dask, bokeh, apache-beam, rechunker, xarray-beam, weatherbench2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "  Attempting uninstall: xarray\n",
      "    Found existing installation: xarray 2024.6.0\n",
      "    Uninstalling xarray-2024.6.0:\n",
      "      Successfully uninstalled xarray-2024.6.0\n",
      "Successfully installed apache-beam-2.56.0 asciitree-0.3.3 bokeh-3.4.1 cloudpickle-2.2.1 crcmod-1.7 dask-2024.6.0 dill-0.3.1.1 docopt-0.6.2 fastavro-1.9.4 fasteners-0.19 grpcio-1.64.1 hdfs-2.7.3 httplib2-0.22.0 immutabledict-4.2.0 importlib-metadata-7.1.0 jax-0.4.30 jaxlib-0.4.30 js2py-0.74 jsonpickle-3.2.1 locket-1.0.0 ml-dtypes-0.4.0 mypy-extensions-1.0.0 numcodecs-0.12.1 objsize-0.7.0 opt-einsum-3.3.0 orjson-3.10.5 pandas-2.0.3 partd-1.4.2 proto-plus-1.23.0 protobuf-4.25.3 pyarrow-14.0.2 pyarrow-hotfix-0.6 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.7.3 rechunker-0.5.2 redis-5.0.6 regex-2024.5.15 tzlocal-5.2 weatherbench2-0.2.0 xarray-2023.7.0 xarray-beam-0.6.3 xyzservices-2024.6.0 zarr-2.18.2 zipp-3.19.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Just for colab (to install torch-scatter, sparse and cluster in a faster way)\n",
    "# See: https://stackoverflow.com/questions/67285115/building-wheels-for-torch-sparse-in-colab-takes-forever\n",
    "import torch  # noqa: F401\n",
    "print(torch.__version__)\n",
    "%pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "%pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "%pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "%pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "%pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "%pip install pyshtools\n",
    "\n",
    "# Install other dependencies (ignore pandas error)\n",
    "%pip install -r requirements.txt\n",
    "%pip install pytest\n",
    "\n",
    "# Install weatherbench\n",
    "%pip install git+https://github.com/google-research/weatherbench2.git\n",
    "\n",
    "# Pip might complain about the Pandas version. The notebook should still work as expected.\n",
    "import apache_beam  # noqa: F401\n",
    "import weatherbench2  # noqa: F401\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e53cd0e-0a4d-407f-9e3c-caa5d583666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
      "Requirement already satisfied: dgl in /opt/conda/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (1.13.1)\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.10/site-packages (from dgl) (3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from dgl) (4.66.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (5.9.0)\n",
      "Requirement already satisfied: torchdata>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (0.7.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from dgl) (1.3.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2023.11.17)\n",
      "Requirement already satisfied: torch>=2 in /opt/conda/lib/python3.10/site-packages (from torchdata>=0.5.0->dgl) (2.3.0+cu121)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->dgl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->dgl) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->pandas->dgl) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata>=0.5.0->dgl) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (2024.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (3.9.5)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (4.4.2)\n",
      "Collecting fsspec==2024.6.0 (from gcsfs)\n",
      "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs) (1.2.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.17.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.32.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.7.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (2023.11.17)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.63.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.23.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.5.0\n",
      "    Uninstalling fsspec-2024.5.0:\n",
      "      Successfully uninstalled fsspec-2024.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.20.0 requires fsspec[http]<=2024.5.0,>=2023.1.0, but you have fsspec 2024.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (2.7.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/lib/python3.10/site-packages (from pydantic) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic) (4.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
    "!pip install gcsfs\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3b9f5ba-46fe-4cf8-a19e-be58c4f44250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h3 (from -r requirements.txt (line 1))\n",
      "  Downloading h3-3.7.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting huggingface-hub (from -r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.8.0)\n",
      "Collecting torch-geometric-temporal (from -r requirements.txt (line 5))\n",
      "  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyshtools in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.12.2)\n",
      "Collecting trimesh (from -r requirements.txt (line 7))\n",
      "  Downloading trimesh-4.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rtree (from -r requirements.txt (line 8))\n",
      "  Downloading Rtree-1.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (1.26.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (0.3.1.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (2.0.3)\n",
      "Collecting requests (from huggingface-hub->-r requirements.txt (line 2))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub->-r requirements.txt (line 2))\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->-r requirements.txt (line 2))\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 3)) (3.9.5)\n",
      "Collecting decorator==4.4.2 (from torch-geometric-temporal->-r requirements.txt (line 5))\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (2.3.0+cu121)\n",
      "Collecting cython (from torch-geometric-temporal->-r requirements.txt (line 5))\n",
      "  Downloading Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting pandas (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch_sparse in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (0.6.18+pt23cu121)\n",
      "Requirement already satisfied: torch_scatter in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: torch_geometric in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch-geometric-temporal->-r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from pyshtools->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /opt/conda/lib/python3.10/site-packages (from pyshtools->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: astropy>=4.0 in /opt/conda/lib/python3.10/site-packages (from pyshtools->-r requirements.txt (line 6)) (6.1.1)\n",
      "Requirement already satisfied: xarray in /opt/conda/lib/python3.10/site-packages (from pyshtools->-r requirements.txt (line 6)) (2023.7.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from pyshtools->-r requirements.txt (line 6)) (1.8.2)\n",
      "Requirement already satisfied: pyerfa>=2.0.1.1 in /opt/conda/lib/python3.10/site-packages (from astropy>=4.0->pyshtools->-r requirements.txt (line 6)) (2.0.1.4)\n",
      "Requirement already satisfied: astropy-iers-data>=0.2024.5.27.0.30.8 in /opt/conda/lib/python3.10/site-packages (from astropy>=4.0->pyshtools->-r requirements.txt (line 6)) (0.2024.6.17.0.31.35)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->pyshtools->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->pyshtools->-r requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->torch-geometric-temporal->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torch-geometric-temporal->-r requirements.txt (line 5)) (12.1.105)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch-geometric-temporal->-r requirements.txt (line 5)) (5.9.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric->torch-geometric-temporal->-r requirements.txt (line 5)) (1.5.0)\n",
      "INFO: pip is looking at multiple versions of xarray to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xarray (from pyshtools->-r requirements.txt (line 6))\n",
      "  Using cached xarray-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2024.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2024.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2024.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2024.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2023.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of xarray to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading xarray-2023.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading xarray-2023.10.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading xarray-2023.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading xarray-2023.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading xarray-2023.8.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading xarray-2023.6.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading xarray-2023.5.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading xarray-2023.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading xarray-2023.4.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading xarray-2023.4.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Downloading xarray-2023.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading xarray-2023.2.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Downloading xarray-2023.1.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torch-geometric-temporal->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric->torch-geometric-temporal->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric->torch-geometric-temporal->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torch-geometric-temporal->-r requirements.txt (line 5)) (1.3.0)\n",
      "Downloading h3-3.7.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Downloading trimesh-4.4.1-py3-none-any.whl (694 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.7/694.7 kB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Rtree-1.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (535 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.2/535.2 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Cython-3.0.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.1/973.1 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torch-geometric-temporal\n",
      "  Building wheel for torch-geometric-temporal (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-geometric-temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86722 sha256=87d87e0ae1314606620c686fc92d1776e18093821c1c919fde1cb66d4f427a95\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n",
      "Successfully built torch-geometric-temporal\n",
      "Installing collected packages: h3, xxhash, trimesh, tqdm, rtree, requests, pyarrow, fsspec, dill, decorator, cython, pandas, multiprocess, huggingface-hub, xarray, torch-geometric-temporal, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "  Attempting uninstall: xarray\n",
      "    Found existing installation: xarray 2023.7.0\n",
      "    Uninstalling xarray-2023.7.0:\n",
      "      Successfully uninstalled xarray-2023.7.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.56.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.56.0 requires pyarrow<15.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "gcsfs 2024.6.0 requires fsspec==2024.6.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "weatherbench2 0.2.0 requires pandas==2.0.3, but you have pandas 1.3.5 which is incompatible.\n",
      "weatherbench2 0.2.0 requires xarray==2023.7.0, but you have xarray 2023.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cython-3.0.10 datasets-2.20.0 decorator-4.4.2 dill-0.3.8 fsspec-2024.5.0 h3-3.7.7 huggingface-hub-0.23.4 multiprocess-0.70.16 pandas-1.3.5 pyarrow-16.1.0 requests-2.32.3 rtree-1.2.0 torch-geometric-temporal-0.54.0 tqdm-4.66.4 trimesh-4.4.1 xarray-2023.1.0 xxhash-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffcf680-4894-423d-843c-2aa5b6730a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/graph_weather_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd graph_weather_repo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739840b9-1714-42fb-b0aa-ccffff390ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph_weather.data.gencast_dataloader import GenCastDataset, BatchedGenCastDataset\n",
    "\n",
    "atmospheric_features = [\n",
    "    \"geopotential\",\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    "    \"u_component_of_wind\",\n",
    "    \"v_component_of_wind\",\n",
    "    \"vertical_velocity\",\n",
    "]\n",
    "single_features = [\n",
    "    \"2m_temperature\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"mean_sea_level_pressure\",\n",
    "    # \"sea_surface_temperature\",\n",
    "    \"total_precipitation_12hr\",\n",
    "]\n",
    "static_features = [\n",
    "    \"geopotential_at_surface\",\n",
    "    \"land_sea_mask\",\n",
    "]\n",
    "\n",
    "#obs_path = \"gs://weatherbench2/datasets/era5/1959-2022-6h-64x32_equiangular_conservative.zarr\"\n",
    "# obs_path = 'gs://weatherbench2/datasets/era5/1959-2022-6h-1440x721.zarr'\n",
    "#obs_path = 'gs://weatherbench2/datasets/era5/1959-2022-6h-512x256_equiangular_conservative.zarr'\n",
    "obs_path = 'gs://weatherbench2/datasets/era5/1959-2022-6h-128x64_equiangular_conservative.zarr'\n",
    "\n",
    "batch_size = 96\n",
    "batched = True\n",
    "if batched:\n",
    "    dataset = BatchedGenCastDataset(\n",
    "    obs_path=obs_path,\n",
    "    atmospheric_features=atmospheric_features,\n",
    "    single_features=single_features,\n",
    "    static_features=static_features,\n",
    "    max_year=2018,\n",
    "    time_step=2,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "    dataloader = dataset\n",
    "else:\n",
    "    dataset = GenCastDataset(\n",
    "        obs_path=obs_path,\n",
    "        atmospheric_features=atmospheric_features,\n",
    "        single_features=single_features,\n",
    "        static_features=static_features,\n",
    "        max_year=2018,\n",
    "        time_step=2,\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "import dask\n",
    "from dask.cache import Cache\n",
    "use_dask_cache = False\n",
    "use_dask_multiworkers = False\n",
    "\n",
    "if use_dask_multiworkers:\n",
    "    dask.config.set(scheduler=\"threads\", num_workers=8)\n",
    "if use_dask_cache:\n",
    "    cache = Cache(1e10)  # 10gb cache\n",
    "    cache.register()\n",
    "dataset = GenCastDataset(\n",
    "        obs_path=obs_path,\n",
    "        atmospheric_features=atmospheric_features,\n",
    "        single_features=single_features,\n",
    "        static_features=static_features,\n",
    "        max_year=2018,\n",
    "        time_step=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da1fd2cf-b59d-49f6-8d8b-2bb596c0ee66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.17.2-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.17.2-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a1fcf4-be22-4c94-8c09-e7ebb50e23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-b\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/graph_weather_repo/wandb/run-20240619_103723-nicqpq7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o' target=\"_blank\">vibrant-durian-10</a></strong> to <a href='https://wandb.ai/g-b/graph_weather_repo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-b/graph_weather_repo' target=\"_blank\">https://wandb.ai/g-b/graph_weather_repo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o' target=\"_blank\">https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4d370c7040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94db1e-dea2-4499-afd4-b129b2fa2edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nicqpq7o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-durian-10</strong> at: <a href='https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o' target=\"_blank\">https://wandb.ai/g-b/graph_weather_repo/runs/nicqpq7o</a><br/> View project at: <a href='https://wandb.ai/g-b/graph_weather_repo' target=\"_blank\">https://wandb.ai/g-b/graph_weather_repo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_103723-nicqpq7o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nicqpq7o). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f0c8dcee224a398e858f0c7950b2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111279112390346, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/graph_weather_repo/wandb/run-20240619_103730-wb0pmrk4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-b/gencast/runs/wb0pmrk4' target=\"_blank\">valiant-music-27</a></strong> to <a href='https://wandb.ai/g-b/gencast' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-b/gencast' target=\"_blank\">https://wandb.ai/g-b/gencast</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-b/gencast/runs/wb0pmrk4' target=\"_blank\">https://wandb.ai/g-b/gencast/runs/wb0pmrk4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/graph_weather_repo/graph_weather/models/gencast/graph/graph_builder.py:305: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  adj_k = adj_k + torch.sparse.mm(adj_k, adj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Iteration 0 of 0 with  training loss 0.12713146209716797.\n",
      "Iteration 16 of 0 with  training loss 0.29574093222618103.\n",
      "Iteration 32 of 0 with  training loss 0.09774332493543625.\n",
      "Iteration 48 of 0 with  training loss 0.10938622057437897.\n",
      "Iteration 64 of 0 with  training loss 0.09434714913368225.\n",
      "Iteration 80 of 0 with  training loss 0.08833733946084976.\n",
      "Iteration 0 of 1 with  training loss 0.08577141165733337.\n",
      "Iteration 16 of 1 with  training loss 0.08602912724018097.\n",
      "Iteration 32 of 1 with  training loss 0.07995927333831787.\n",
      "Iteration 48 of 1 with  training loss 0.08706379681825638.\n",
      "Iteration 64 of 1 with  training loss 0.08741670846939087.\n",
      "Iteration 80 of 1 with  training loss 0.08472777903079987.\n",
      "Iteration 0 of 2 with  training loss 0.0809980109333992.\n",
      "Iteration 16 of 2 with  training loss 0.07554333657026291.\n",
      "Iteration 32 of 2 with  training loss 0.07508508861064911.\n",
      "Iteration 48 of 2 with  training loss 0.0826520323753357.\n",
      "Iteration 64 of 2 with  training loss 0.07671736925840378.\n",
      "Iteration 80 of 2 with  training loss 0.07930689305067062.\n",
      "Iteration 0 of 3 with  training loss 0.07586632668972015.\n",
      "Iteration 16 of 3 with  training loss 0.07963509857654572.\n",
      "Iteration 32 of 3 with  training loss 0.07353159785270691.\n",
      "Iteration 48 of 3 with  training loss 0.06977810710668564.\n",
      "Iteration 64 of 3 with  training loss 0.07191520184278488.\n",
      "Iteration 80 of 3 with  training loss 0.07745625078678131.\n",
      "Iteration 0 of 4 with  training loss 0.0713835209608078.\n",
      "Iteration 16 of 4 with  training loss 0.07505984604358673.\n",
      "Iteration 32 of 4 with  training loss 0.0712507963180542.\n",
      "Iteration 48 of 4 with  training loss 0.06972921639680862.\n",
      "Iteration 64 of 4 with  training loss 0.07332345843315125.\n",
      "Iteration 80 of 4 with  training loss 0.07137187570333481.\n",
      "Iteration 0 of 5 with  training loss 0.06972844153642654.\n",
      "Iteration 16 of 5 with  training loss 0.06971736252307892.\n",
      "Iteration 32 of 5 with  training loss 0.07059867680072784.\n",
      "Iteration 48 of 5 with  training loss 0.06538226455450058.\n",
      "Iteration 64 of 5 with  training loss 0.06580445170402527.\n",
      "Iteration 80 of 5 with  training loss 0.068601593375206.\n",
      "Iteration 0 of 6 with  training loss 0.06526759266853333.\n",
      "Iteration 16 of 6 with  training loss 0.06374214589595795.\n",
      "Iteration 32 of 6 with  training loss 0.0654897689819336.\n",
      "Iteration 48 of 6 with  training loss 0.06720845401287079.\n",
      "Iteration 64 of 6 with  training loss 0.06796642392873764.\n",
      "Iteration 80 of 6 with  training loss 0.06504073739051819.\n",
      "Iteration 0 of 7 with  training loss 0.06783325970172882.\n",
      "Iteration 16 of 7 with  training loss 0.06652866303920746.\n",
      "Iteration 32 of 7 with  training loss 0.06492753326892853.\n",
      "Iteration 48 of 7 with  training loss 0.06773468852043152.\n",
      "Iteration 64 of 7 with  training loss 0.06501580774784088.\n",
      "Iteration 80 of 7 with  training loss 0.06319932639598846.\n",
      "Iteration 0 of 8 with  training loss 0.061829574406147.\n",
      "Iteration 16 of 8 with  training loss 0.06428033113479614.\n",
      "Iteration 32 of 8 with  training loss 0.0651252269744873.\n",
      "Iteration 48 of 8 with  training loss 0.06279656291007996.\n",
      "Iteration 64 of 8 with  training loss 0.06033895164728165.\n",
      "Iteration 80 of 8 with  training loss 0.05833927169442177.\n",
      "Iteration 0 of 9 with  training loss 0.060544759035110474.\n",
      "Iteration 16 of 9 with  training loss 0.06171617656946182.\n",
      "Iteration 32 of 9 with  training loss 0.06321275979280472.\n",
      "Iteration 48 of 9 with  training loss 0.054562799632549286.\n",
      "Iteration 64 of 9 with  training loss 0.055703938007354736.\n",
      "Iteration 80 of 9 with  training loss 0.05898695066571236.\n",
      "Iteration 0 of 10 with  training loss 0.06192789971828461.\n",
      "Iteration 16 of 10 with  training loss 0.0554104745388031.\n",
      "Iteration 32 of 10 with  training loss 0.06438443064689636.\n",
      "Iteration 48 of 10 with  training loss 0.06368961930274963.\n",
      "Iteration 64 of 10 with  training loss 0.06140067055821419.\n",
      "Iteration 80 of 10 with  training loss 0.06049579009413719.\n",
      "Iteration 0 of 11 with  training loss 0.06065291166305542.\n",
      "Iteration 16 of 11 with  training loss 0.06251934170722961.\n",
      "Iteration 32 of 11 with  training loss 0.059368573129177094.\n",
      "Iteration 48 of 11 with  training loss 0.0606161430478096.\n",
      "Iteration 64 of 11 with  training loss 0.06078071892261505.\n",
      "Iteration 80 of 11 with  training loss 0.060778647661209106.\n",
      "Iteration 0 of 12 with  training loss 0.06142031401395798.\n",
      "Iteration 16 of 12 with  training loss 0.06460650265216827.\n",
      "Iteration 32 of 12 with  training loss 0.06044263392686844.\n",
      "Iteration 48 of 12 with  training loss 0.05834227427840233.\n",
      "Iteration 64 of 12 with  training loss 0.06132063269615173.\n",
      "Iteration 80 of 12 with  training loss 0.05911729484796524.\n",
      "Iteration 0 of 13 with  training loss 0.05690228193998337.\n",
      "Iteration 16 of 13 with  training loss 0.05802535265684128.\n",
      "Iteration 32 of 13 with  training loss 0.05540502816438675.\n",
      "Iteration 48 of 13 with  training loss 0.05843297764658928.\n",
      "Iteration 64 of 13 with  training loss 0.06066382676362991.\n",
      "Iteration 80 of 13 with  training loss 0.05725829303264618.\n",
      "Iteration 0 of 14 with  training loss 0.05662142112851143.\n",
      "Iteration 16 of 14 with  training loss 0.057374849915504456.\n",
      "Iteration 32 of 14 with  training loss 0.05543152987957001.\n",
      "Iteration 48 of 14 with  training loss 0.05654134601354599.\n",
      "Iteration 64 of 14 with  training loss 0.05653180181980133.\n",
      "Iteration 80 of 14 with  training loss 0.05650506913661957.\n",
      "Iteration 0 of 15 with  training loss 0.05848119035363197.\n",
      "Iteration 16 of 15 with  training loss 0.059720851480960846.\n",
      "Iteration 32 of 15 with  training loss 0.05640371888875961.\n",
      "Iteration 48 of 15 with  training loss 0.05698305368423462.\n",
      "Iteration 64 of 15 with  training loss 0.05568154156208038.\n",
      "Iteration 80 of 15 with  training loss 0.05951394885778427.\n",
      "Iteration 0 of 16 with  training loss 0.056028708815574646.\n",
      "Iteration 16 of 16 with  training loss 0.05087724328041077.\n",
      "Iteration 32 of 16 with  training loss 0.05314230918884277.\n",
      "Iteration 48 of 16 with  training loss 0.05636683106422424.\n",
      "Iteration 64 of 16 with  training loss 0.055813584476709366.\n",
      "Iteration 80 of 16 with  training loss 0.056738488376140594.\n",
      "Iteration 0 of 17 with  training loss 0.057206712663173676.\n",
      "Iteration 16 of 17 with  training loss 0.05790585279464722.\n",
      "Iteration 32 of 17 with  training loss 0.05397520586848259.\n",
      "Iteration 48 of 17 with  training loss 0.05328172445297241.\n",
      "Iteration 64 of 17 with  training loss 0.05392112582921982.\n",
      "Iteration 80 of 17 with  training loss 0.05167123302817345.\n",
      "Iteration 0 of 18 with  training loss 0.053804896771907806.\n",
      "Iteration 16 of 18 with  training loss 0.05177538841962814.\n",
      "Iteration 32 of 18 with  training loss 0.054479241371154785.\n",
      "Iteration 48 of 18 with  training loss 0.05798843130469322.\n",
      "Iteration 64 of 18 with  training loss 0.05738573521375656.\n",
      "Iteration 80 of 18 with  training loss 0.05498165637254715.\n",
      "Iteration 0 of 19 with  training loss 0.056369148194789886.\n",
      "Iteration 16 of 19 with  training loss 0.052284471690654755.\n",
      "Iteration 32 of 19 with  training loss 0.05064941942691803.\n",
      "Iteration 48 of 19 with  training loss 0.05016385018825531.\n",
      "Iteration 64 of 19 with  training loss 0.04983730614185333.\n",
      "Iteration 80 of 19 with  training loss 0.04802834242582321.\n",
      "Iteration 0 of 20 with  training loss 0.049998290836811066.\n",
      "Iteration 16 of 20 with  training loss 0.05269211530685425.\n",
      "Iteration 32 of 20 with  training loss 0.047865018248558044.\n",
      "Iteration 48 of 20 with  training loss 0.04765821248292923.\n",
      "Iteration 64 of 20 with  training loss 0.049492716789245605.\n",
      "Iteration 80 of 20 with  training loss 0.05197075009346008.\n",
      "Iteration 0 of 21 with  training loss 0.051736652851104736.\n",
      "Iteration 16 of 21 with  training loss 0.055179573595523834.\n",
      "Iteration 32 of 21 with  training loss 0.05290325731039047.\n",
      "Iteration 48 of 21 with  training loss 0.04677531123161316.\n",
      "Iteration 64 of 21 with  training loss 0.04776280000805855.\n",
      "Iteration 80 of 21 with  training loss 0.04836457967758179.\n",
      "Iteration 0 of 22 with  training loss 0.047377996146678925.\n",
      "Iteration 16 of 22 with  training loss 0.04982168972492218.\n",
      "Iteration 32 of 22 with  training loss 0.0500725656747818.\n",
      "Iteration 48 of 22 with  training loss 0.053497977554798126.\n",
      "Iteration 64 of 22 with  training loss 0.05107010155916214.\n",
      "Iteration 80 of 22 with  training loss 0.05069877579808235.\n",
      "Iteration 0 of 23 with  training loss 0.0524868406355381.\n",
      "Iteration 16 of 23 with  training loss 0.050694823265075684.\n",
      "Iteration 32 of 23 with  training loss 0.04744786024093628.\n",
      "Iteration 48 of 23 with  training loss 0.053177352994680405.\n",
      "Iteration 64 of 23 with  training loss 0.04827379435300827.\n",
      "Iteration 80 of 23 with  training loss 0.049750737845897675.\n",
      "Iteration 0 of 24 with  training loss 0.050643786787986755.\n",
      "Iteration 16 of 24 with  training loss 0.05054539814591408.\n",
      "Iteration 32 of 24 with  training loss 0.046799931675195694.\n",
      "Iteration 48 of 24 with  training loss 0.05074414610862732.\n",
      "Iteration 64 of 24 with  training loss 0.05014745891094208.\n",
      "Iteration 80 of 24 with  training loss 0.048370156437158585.\n",
      "Iteration 0 of 25 with  training loss 0.0530693456530571.\n",
      "Iteration 16 of 25 with  training loss 0.0503256618976593.\n",
      "Iteration 32 of 25 with  training loss 0.04673602432012558.\n",
      "Iteration 48 of 25 with  training loss 0.0509481281042099.\n",
      "Iteration 64 of 25 with  training loss 0.05444571375846863.\n",
      "Iteration 80 of 25 with  training loss 0.05011524632573128.\n",
      "Iteration 0 of 26 with  training loss 0.04923386126756668.\n",
      "Iteration 16 of 26 with  training loss 0.051276542246341705.\n",
      "Iteration 32 of 26 with  training loss 0.05323926359415054.\n",
      "Iteration 48 of 26 with  training loss 0.05057869851589203.\n",
      "Iteration 64 of 26 with  training loss 0.04953986406326294.\n",
      "Iteration 80 of 26 with  training loss 0.05288964509963989.\n",
      "Iteration 0 of 27 with  training loss 0.04969959333539009.\n",
      "Iteration 16 of 27 with  training loss 0.05019817501306534.\n",
      "Iteration 32 of 27 with  training loss 0.05285073071718216.\n",
      "Iteration 48 of 27 with  training loss 0.05022522062063217.\n",
      "Iteration 64 of 27 with  training loss 0.051644664257764816.\n",
      "Iteration 80 of 27 with  training loss 0.051508307456970215.\n",
      "Iteration 0 of 28 with  training loss 0.05069803446531296.\n",
      "Iteration 16 of 28 with  training loss 0.05158035829663277.\n",
      "Iteration 32 of 28 with  training loss 0.04764395207166672.\n",
      "Iteration 48 of 28 with  training loss 0.05219842493534088.\n",
      "Iteration 64 of 28 with  training loss 0.049385085701942444.\n",
      "Iteration 80 of 28 with  training loss 0.04661472886800766.\n",
      "Iteration 0 of 29 with  training loss 0.04958043992519379.\n",
      "Iteration 16 of 29 with  training loss 0.051357775926589966.\n",
      "Iteration 32 of 29 with  training loss 0.049840666353702545.\n",
      "Iteration 48 of 29 with  training loss 0.05164298042654991.\n",
      "Iteration 64 of 29 with  training loss 0.04750734567642212.\n",
      "Iteration 80 of 29 with  training loss 0.048469267785549164.\n",
      "Iteration 0 of 30 with  training loss 0.04830201715230942.\n",
      "Iteration 16 of 30 with  training loss 0.050078995525836945.\n",
      "Iteration 32 of 30 with  training loss 0.045645225793123245.\n",
      "Iteration 48 of 30 with  training loss 0.050048209726810455.\n",
      "Iteration 64 of 30 with  training loss 0.04613951966166496.\n",
      "Iteration 80 of 30 with  training loss 0.04592505842447281.\n",
      "Iteration 0 of 31 with  training loss 0.049156554043293.\n",
      "Iteration 16 of 31 with  training loss 0.053452860563993454.\n",
      "Iteration 32 of 31 with  training loss 0.05367801710963249.\n",
      "Iteration 48 of 31 with  training loss 0.050555288791656494.\n",
      "Iteration 64 of 31 with  training loss 0.04552187770605087.\n",
      "Iteration 80 of 31 with  training loss 0.04796876758337021.\n",
      "Iteration 0 of 32 with  training loss 0.04878659173846245.\n",
      "Iteration 16 of 32 with  training loss 0.04719287157058716.\n",
      "Iteration 32 of 32 with  training loss 0.0473468154668808.\n",
      "Iteration 48 of 32 with  training loss 0.047923777252435684.\n",
      "Iteration 64 of 32 with  training loss 0.05090837925672531.\n",
      "Iteration 80 of 32 with  training loss 0.04860794544219971.\n",
      "Iteration 0 of 33 with  training loss 0.046345580369234085.\n",
      "Iteration 16 of 33 with  training loss 0.05258439481258392.\n",
      "Iteration 32 of 33 with  training loss 0.045339249074459076.\n",
      "Iteration 48 of 33 with  training loss 0.052555449306964874.\n",
      "Iteration 64 of 33 with  training loss 0.05029035732150078.\n",
      "Iteration 80 of 33 with  training loss 0.05292588472366333.\n",
      "Iteration 0 of 34 with  training loss 0.05027409642934799.\n",
      "Iteration 16 of 34 with  training loss 0.048626046627759933.\n",
      "Iteration 32 of 34 with  training loss 0.049531906843185425.\n",
      "Iteration 48 of 34 with  training loss 0.05032649636268616.\n",
      "Iteration 64 of 34 with  training loss 0.04660550877451897.\n",
      "Iteration 80 of 34 with  training loss 0.04905177652835846.\n",
      "Iteration 0 of 35 with  training loss 0.048460833728313446.\n",
      "Iteration 16 of 35 with  training loss 0.04662378877401352.\n",
      "Iteration 32 of 35 with  training loss 0.04588507488369942.\n",
      "Iteration 48 of 35 with  training loss 0.04628074914216995.\n",
      "Iteration 64 of 35 with  training loss 0.046081412583589554.\n",
      "Iteration 80 of 35 with  training loss 0.04540015384554863.\n",
      "Iteration 0 of 36 with  training loss 0.04487830400466919.\n",
      "Iteration 16 of 36 with  training loss 0.05010616034269333.\n",
      "Iteration 32 of 36 with  training loss 0.05071371793746948.\n",
      "Iteration 48 of 36 with  training loss 0.04721079766750336.\n",
      "Iteration 64 of 36 with  training loss 0.047795429825782776.\n",
      "Iteration 80 of 36 with  training loss 0.050286658108234406.\n",
      "Iteration 0 of 37 with  training loss 0.04452478140592575.\n",
      "Iteration 16 of 37 with  training loss 0.047716252505779266.\n",
      "Iteration 32 of 37 with  training loss 0.047430090606212616.\n",
      "Iteration 48 of 37 with  training loss 0.04645516350865364.\n",
      "Iteration 64 of 37 with  training loss 0.04687118902802467.\n",
      "Iteration 80 of 37 with  training loss 0.044809263199567795.\n",
      "Iteration 0 of 38 with  training loss 0.043467987328767776.\n",
      "Iteration 16 of 38 with  training loss 0.04861830174922943.\n",
      "Iteration 32 of 38 with  training loss 0.04339639097452164.\n",
      "Iteration 48 of 38 with  training loss 0.04450913891196251.\n",
      "Iteration 64 of 38 with  training loss 0.045184507966041565.\n",
      "Iteration 80 of 38 with  training loss 0.04510532319545746.\n",
      "Iteration 0 of 39 with  training loss 0.04744039848446846.\n",
      "Iteration 16 of 39 with  training loss 0.04633504897356033.\n",
      "Iteration 32 of 39 with  training loss 0.048443909734487534.\n",
      "Iteration 48 of 39 with  training loss 0.04769942909479141.\n",
      "Iteration 64 of 39 with  training loss 0.04875478520989418.\n",
      "Iteration 80 of 39 with  training loss 0.0465552881360054.\n",
      "Iteration 0 of 40 with  training loss 0.046094246208667755.\n",
      "Iteration 16 of 40 with  training loss 0.04824769124388695.\n",
      "Iteration 32 of 40 with  training loss 0.051857054233551025.\n",
      "Iteration 48 of 40 with  training loss 0.05028141289949417.\n",
      "Iteration 64 of 40 with  training loss 0.05026542395353317.\n",
      "Iteration 80 of 40 with  training loss 0.04543827474117279.\n",
      "Iteration 0 of 41 with  training loss 0.04914120212197304.\n",
      "Iteration 16 of 41 with  training loss 0.048247940838336945.\n",
      "Iteration 32 of 41 with  training loss 0.044719524681568146.\n",
      "Iteration 48 of 41 with  training loss 0.05277034640312195.\n",
      "Iteration 64 of 41 with  training loss 0.05270276218652725.\n",
      "Iteration 80 of 41 with  training loss 0.04753322899341583.\n",
      "Iteration 0 of 42 with  training loss 0.04639401286840439.\n",
      "Iteration 16 of 42 with  training loss 0.05037307366728783.\n",
      "Iteration 32 of 42 with  training loss 0.04672214388847351.\n",
      "Iteration 48 of 42 with  training loss 0.049106474965810776.\n",
      "Iteration 64 of 42 with  training loss 0.04733583703637123.\n",
      "Iteration 80 of 42 with  training loss 0.045198820531368256.\n",
      "Iteration 0 of 43 with  training loss 0.04392861947417259.\n",
      "Iteration 16 of 43 with  training loss 0.04813925921916962.\n",
      "Iteration 32 of 43 with  training loss 0.0469936728477478.\n",
      "Iteration 48 of 43 with  training loss 0.04514753818511963.\n",
      "Iteration 64 of 43 with  training loss 0.04828193411231041.\n",
      "Iteration 80 of 43 with  training loss 0.04679036885499954.\n",
      "Iteration 0 of 44 with  training loss 0.046564728021621704.\n",
      "Iteration 16 of 44 with  training loss 0.044793419539928436.\n",
      "Iteration 32 of 44 with  training loss 0.044338565319776535.\n",
      "Iteration 48 of 44 with  training loss 0.046612758189439774.\n",
      "Iteration 64 of 44 with  training loss 0.04484114795923233.\n",
      "Iteration 80 of 44 with  training loss 0.047745708376169205.\n",
      "Iteration 0 of 45 with  training loss 0.04747159034013748.\n",
      "Iteration 16 of 45 with  training loss 0.044267166405916214.\n",
      "Iteration 32 of 45 with  training loss 0.047050558030605316.\n",
      "Iteration 48 of 45 with  training loss 0.04697993025183678.\n",
      "Iteration 64 of 45 with  training loss 0.049376480281353.\n",
      "Iteration 80 of 45 with  training loss 0.04394080117344856.\n",
      "Iteration 0 of 46 with  training loss 0.04225613921880722.\n",
      "Iteration 16 of 46 with  training loss 0.04990937560796738.\n",
      "Iteration 32 of 46 with  training loss 0.045659326016902924.\n",
      "Iteration 48 of 46 with  training loss 0.04685727134346962.\n",
      "Iteration 64 of 46 with  training loss 0.050074461847543716.\n",
      "Iteration 80 of 46 with  training loss 0.047139979898929596.\n",
      "Iteration 0 of 47 with  training loss 0.04560552164912224.\n",
      "Iteration 16 of 47 with  training loss 0.047291189432144165.\n",
      "Iteration 32 of 47 with  training loss 0.047647491097450256.\n",
      "Iteration 48 of 47 with  training loss 0.05161972716450691.\n",
      "Iteration 64 of 47 with  training loss 0.04932229593396187.\n",
      "Iteration 80 of 47 with  training loss 0.04925290495157242.\n",
      "Iteration 0 of 48 with  training loss 0.046865977346897125.\n",
      "Iteration 16 of 48 with  training loss 0.045761387795209885.\n",
      "Iteration 32 of 48 with  training loss 0.048441316932439804.\n",
      "Iteration 48 of 48 with  training loss 0.048887789249420166.\n",
      "Iteration 64 of 48 with  training loss 0.04644579812884331.\n",
      "Iteration 80 of 48 with  training loss 0.04971335083246231.\n",
      "Iteration 0 of 49 with  training loss 0.04591691121459007.\n",
      "Iteration 16 of 49 with  training loss 0.042866237461566925.\n",
      "Iteration 32 of 49 with  training loss 0.04233803600072861.\n",
      "Iteration 48 of 49 with  training loss 0.045847952365875244.\n",
      "Iteration 64 of 49 with  training loss 0.04510204493999481.\n",
      "Iteration 80 of 49 with  training loss 0.046854641288518906.\n",
      "Iteration 0 of 50 with  training loss 0.04749336838722229.\n",
      "Iteration 16 of 50 with  training loss 0.047926679253578186.\n",
      "Iteration 32 of 50 with  training loss 0.04746944457292557.\n",
      "Iteration 48 of 50 with  training loss 0.0439583994448185.\n",
      "Iteration 64 of 50 with  training loss 0.046466290950775146.\n",
      "Iteration 80 of 50 with  training loss 0.044145889580249786.\n",
      "Iteration 0 of 51 with  training loss 0.042664095759391785.\n",
      "Iteration 16 of 51 with  training loss 0.04552357643842697.\n",
      "Iteration 32 of 51 with  training loss 0.0448659248650074.\n",
      "Iteration 48 of 51 with  training loss 0.04249195381999016.\n",
      "Iteration 64 of 51 with  training loss 0.04558958858251572.\n",
      "Iteration 80 of 51 with  training loss 0.045975133776664734.\n",
      "Iteration 0 of 52 with  training loss 0.04520239681005478.\n",
      "Iteration 16 of 52 with  training loss 0.04459606856107712.\n",
      "Iteration 32 of 52 with  training loss 0.04360402375459671.\n",
      "Iteration 48 of 52 with  training loss 0.048125606030225754.\n",
      "Iteration 64 of 52 with  training loss 0.04546424001455307.\n",
      "Iteration 80 of 52 with  training loss 0.04543366655707359.\n",
      "Iteration 0 of 53 with  training loss 0.04467584565281868.\n",
      "Iteration 16 of 53 with  training loss 0.04651074856519699.\n",
      "Iteration 32 of 53 with  training loss 0.045425109565258026.\n",
      "Iteration 48 of 53 with  training loss 0.04416678100824356.\n",
      "Iteration 64 of 53 with  training loss 0.046160951256752014.\n",
      "Iteration 80 of 53 with  training loss 0.04581868276000023.\n",
      "Iteration 0 of 54 with  training loss 0.047105900943279266.\n",
      "Iteration 16 of 54 with  training loss 0.046272244304418564.\n",
      "Iteration 32 of 54 with  training loss 0.04473254829645157.\n",
      "Iteration 48 of 54 with  training loss 0.045508407056331635.\n",
      "Iteration 64 of 54 with  training loss 0.04508808255195618.\n",
      "Iteration 80 of 54 with  training loss 0.042258717119693756.\n",
      "Iteration 0 of 55 with  training loss 0.04395928233861923.\n",
      "Iteration 16 of 55 with  training loss 0.03928793594241142.\n",
      "Iteration 32 of 55 with  training loss 0.043067269027233124.\n",
      "Iteration 48 of 55 with  training loss 0.044782690703868866.\n",
      "Iteration 64 of 55 with  training loss 0.0437653586268425.\n",
      "Iteration 80 of 55 with  training loss 0.0441107302904129.\n",
      "Iteration 0 of 56 with  training loss 0.04623730853199959.\n",
      "Iteration 16 of 56 with  training loss 0.042778752744197845.\n",
      "Iteration 32 of 56 with  training loss 0.043184660375118256.\n",
      "Iteration 48 of 56 with  training loss 0.040522582828998566.\n",
      "Iteration 64 of 56 with  training loss 0.04348696395754814.\n",
      "Iteration 80 of 56 with  training loss 0.04673418402671814.\n",
      "Iteration 0 of 57 with  training loss 0.045190855860710144.\n",
      "Iteration 16 of 57 with  training loss 0.04202249273657799.\n",
      "Iteration 32 of 57 with  training loss 0.0461115725338459.\n",
      "Iteration 48 of 57 with  training loss 0.041274674236774445.\n",
      "Iteration 64 of 57 with  training loss 0.04314391314983368.\n",
      "Iteration 80 of 57 with  training loss 0.04102773219347.\n",
      "Iteration 0 of 58 with  training loss 0.04775816574692726.\n",
      "Iteration 16 of 58 with  training loss 0.04604729637503624.\n",
      "Iteration 32 of 58 with  training loss 0.04108747839927673.\n",
      "Iteration 48 of 58 with  training loss 0.04437365382909775.\n",
      "Iteration 64 of 58 with  training loss 0.04642079770565033.\n",
      "Iteration 80 of 58 with  training loss 0.04355495423078537.\n",
      "Iteration 0 of 59 with  training loss 0.04752000421285629.\n",
      "Iteration 16 of 59 with  training loss 0.04668191075325012.\n",
      "Iteration 32 of 59 with  training loss 0.04157187044620514.\n",
      "Iteration 48 of 59 with  training loss 0.04485541582107544.\n",
      "Iteration 64 of 59 with  training loss 0.04563131183385849.\n",
      "Iteration 80 of 59 with  training loss 0.04677059128880501.\n",
      "Iteration 0 of 60 with  training loss 0.04416033253073692.\n",
      "Iteration 16 of 60 with  training loss 0.043439045548439026.\n",
      "Iteration 32 of 60 with  training loss 0.0467044897377491.\n",
      "Iteration 48 of 60 with  training loss 0.04314807802438736.\n",
      "Iteration 64 of 60 with  training loss 0.04652827978134155.\n",
      "Iteration 80 of 60 with  training loss 0.0417298823595047.\n",
      "Iteration 0 of 61 with  training loss 0.043068017810583115.\n",
      "Iteration 16 of 61 with  training loss 0.04349815100431442.\n",
      "Iteration 32 of 61 with  training loss 0.04686179384589195.\n",
      "Iteration 48 of 61 with  training loss 0.04343016445636749.\n",
      "Iteration 64 of 61 with  training loss 0.05060184746980667.\n",
      "Iteration 80 of 61 with  training loss 0.04933866113424301.\n",
      "Iteration 0 of 62 with  training loss 0.0420616939663887.\n",
      "Iteration 16 of 62 with  training loss 0.04337087646126747.\n",
      "Iteration 32 of 62 with  training loss 0.04635261744260788.\n",
      "Iteration 48 of 62 with  training loss 0.04116536304354668.\n",
      "Iteration 64 of 62 with  training loss 0.038345709443092346.\n",
      "Iteration 80 of 62 with  training loss 0.046634748578071594.\n",
      "Iteration 0 of 63 with  training loss 0.04671734198927879.\n",
      "Iteration 16 of 63 with  training loss 0.048009954392910004.\n",
      "Iteration 32 of 63 with  training loss 0.04480986297130585.\n",
      "Iteration 48 of 63 with  training loss 0.04643142968416214.\n",
      "Iteration 64 of 63 with  training loss 0.046945326030254364.\n",
      "Iteration 80 of 63 with  training loss 0.04185745492577553.\n",
      "Iteration 0 of 64 with  training loss 0.043469078838825226.\n",
      "Iteration 16 of 64 with  training loss 0.041991934180259705.\n",
      "Iteration 32 of 64 with  training loss 0.04849851876497269.\n",
      "Iteration 48 of 64 with  training loss 0.045809365808963776.\n",
      "Iteration 64 of 64 with  training loss 0.0444728285074234.\n",
      "Iteration 80 of 64 with  training loss 0.04704466834664345.\n",
      "Iteration 0 of 65 with  training loss 0.039347581565380096.\n",
      "Iteration 16 of 65 with  training loss 0.040897637605667114.\n",
      "Iteration 32 of 65 with  training loss 0.04483668878674507.\n",
      "Iteration 48 of 65 with  training loss 0.04338698089122772.\n",
      "Iteration 64 of 65 with  training loss 0.04423398897051811.\n",
      "Iteration 80 of 65 with  training loss 0.042780838906764984.\n",
      "Iteration 0 of 66 with  training loss 0.044387832283973694.\n",
      "Iteration 16 of 66 with  training loss 0.04389791190624237.\n",
      "Iteration 32 of 66 with  training loss 0.042915791273117065.\n",
      "Iteration 48 of 66 with  training loss 0.04068939387798309.\n",
      "Iteration 64 of 66 with  training loss 0.04635760188102722.\n",
      "Iteration 80 of 66 with  training loss 0.04313790425658226.\n",
      "Iteration 0 of 67 with  training loss 0.03996945917606354.\n",
      "Iteration 16 of 67 with  training loss 0.04152553528547287.\n",
      "Iteration 32 of 67 with  training loss 0.04248076677322388.\n",
      "Iteration 48 of 67 with  training loss 0.04281609505414963.\n",
      "Iteration 64 of 67 with  training loss 0.03868221491575241.\n",
      "Iteration 80 of 67 with  training loss 0.045076027512550354.\n",
      "Iteration 0 of 68 with  training loss 0.04583564028143883.\n",
      "Iteration 16 of 68 with  training loss 0.04157615825533867.\n",
      "Iteration 32 of 68 with  training loss 0.03723640367388725.\n",
      "Iteration 48 of 68 with  training loss 0.04387986660003662.\n",
      "Iteration 64 of 68 with  training loss 0.03941013664007187.\n",
      "Iteration 80 of 68 with  training loss 0.04487888887524605.\n",
      "Iteration 0 of 69 with  training loss 0.04122212901711464.\n",
      "Iteration 16 of 69 with  training loss 0.046961672604084015.\n",
      "Iteration 32 of 69 with  training loss 0.04231496527791023.\n",
      "Iteration 48 of 69 with  training loss 0.03698785975575447.\n",
      "Iteration 64 of 69 with  training loss 0.04353369027376175.\n",
      "Iteration 80 of 69 with  training loss 0.03940695524215698.\n",
      "Iteration 0 of 70 with  training loss 0.041745930910110474.\n",
      "Iteration 16 of 70 with  training loss 0.04112894833087921.\n",
      "Iteration 32 of 70 with  training loss 0.04050537943840027.\n",
      "Iteration 48 of 70 with  training loss 0.041969940066337585.\n",
      "Iteration 64 of 70 with  training loss 0.04558531567454338.\n",
      "Iteration 80 of 70 with  training loss 0.0428917221724987.\n",
      "Iteration 0 of 71 with  training loss 0.04213309288024902.\n",
      "Iteration 16 of 71 with  training loss 0.04075254127383232.\n",
      "Iteration 32 of 71 with  training loss 0.04013088345527649.\n",
      "Iteration 48 of 71 with  training loss 0.04470251500606537.\n",
      "Iteration 64 of 71 with  training loss 0.040712304413318634.\n",
      "Iteration 80 of 71 with  training loss 0.04163075238466263.\n",
      "Iteration 0 of 72 with  training loss 0.04313161224126816.\n",
      "Iteration 16 of 72 with  training loss 0.04498698189854622.\n",
      "Iteration 32 of 72 with  training loss 0.040325406938791275.\n",
      "Iteration 48 of 72 with  training loss 0.04365171119570732.\n",
      "Iteration 64 of 72 with  training loss 0.04465014487504959.\n",
      "Iteration 80 of 72 with  training loss 0.04361102357506752.\n",
      "Iteration 0 of 73 with  training loss 0.04307053983211517.\n",
      "Iteration 16 of 73 with  training loss 0.039703331887722015.\n",
      "Iteration 32 of 73 with  training loss 0.040812186896800995.\n",
      "Iteration 48 of 73 with  training loss 0.04393412917852402.\n",
      "Iteration 64 of 73 with  training loss 0.042477305978536606.\n",
      "Iteration 80 of 73 with  training loss 0.04138055071234703.\n",
      "Iteration 0 of 74 with  training loss 0.038458872586488724.\n",
      "Iteration 16 of 74 with  training loss 0.04461585730314255.\n",
      "Iteration 32 of 74 with  training loss 0.045060787349939346.\n",
      "Iteration 48 of 74 with  training loss 0.040297046303749084.\n",
      "Iteration 64 of 74 with  training loss 0.04354152828454971.\n",
      "Iteration 80 of 74 with  training loss 0.04113783314824104.\n",
      "Iteration 0 of 75 with  training loss 0.04229164496064186.\n",
      "Iteration 16 of 75 with  training loss 0.04592733085155487.\n",
      "Iteration 32 of 75 with  training loss 0.042140815407037735.\n",
      "Iteration 48 of 75 with  training loss 0.046247199177742004.\n",
      "Iteration 64 of 75 with  training loss 0.04382983595132828.\n",
      "Iteration 80 of 75 with  training loss 0.0429132804274559.\n",
      "Iteration 0 of 76 with  training loss 0.04280874878168106.\n",
      "Iteration 16 of 76 with  training loss 0.04053208604454994.\n",
      "Iteration 32 of 76 with  training loss 0.04410210996866226.\n",
      "Iteration 48 of 76 with  training loss 0.04776864871382713.\n",
      "Iteration 64 of 76 with  training loss 0.04649335518479347.\n",
      "Iteration 80 of 76 with  training loss 0.04121384024620056.\n",
      "Iteration 0 of 77 with  training loss 0.04155181348323822.\n",
      "Iteration 16 of 77 with  training loss 0.04214027523994446.\n",
      "Iteration 32 of 77 with  training loss 0.041086092591285706.\n",
      "Iteration 48 of 77 with  training loss 0.04351635277271271.\n",
      "Iteration 64 of 77 with  training loss 0.04282177984714508.\n",
      "Iteration 80 of 77 with  training loss 0.043109625577926636.\n",
      "Iteration 0 of 78 with  training loss 0.04116399213671684.\n",
      "Iteration 16 of 78 with  training loss 0.03871062397956848.\n",
      "Iteration 32 of 78 with  training loss 0.047086454927921295.\n",
      "Iteration 48 of 78 with  training loss 0.04250594228506088.\n",
      "Iteration 64 of 78 with  training loss 0.042971085757017136.\n",
      "Iteration 80 of 78 with  training loss 0.044348135590553284.\n",
      "Iteration 0 of 79 with  training loss 0.0427282378077507.\n",
      "Iteration 16 of 79 with  training loss 0.04698082432150841.\n",
      "Iteration 32 of 79 with  training loss 0.04363449662923813.\n",
      "Iteration 48 of 79 with  training loss 0.043656669557094574.\n",
      "Iteration 64 of 79 with  training loss 0.04534833878278732.\n",
      "Iteration 80 of 79 with  training loss 0.04458292946219444.\n",
      "Iteration 0 of 80 with  training loss 0.044514745473861694.\n",
      "Iteration 16 of 80 with  training loss 0.03920101374387741.\n",
      "Iteration 32 of 80 with  training loss 0.04107654094696045.\n",
      "Iteration 48 of 80 with  training loss 0.03854092210531235.\n",
      "Iteration 64 of 80 with  training loss 0.03868161141872406.\n",
      "Iteration 80 of 80 with  training loss 0.0433683767914772.\n",
      "Iteration 0 of 81 with  training loss 0.0388590507209301.\n",
      "Iteration 16 of 81 with  training loss 0.043412912636995316.\n",
      "Iteration 32 of 81 with  training loss 0.04138391464948654.\n",
      "Iteration 48 of 81 with  training loss 0.0423370823264122.\n",
      "Iteration 64 of 81 with  training loss 0.042376913130283356.\n",
      "Iteration 80 of 81 with  training loss 0.04007869213819504.\n",
      "Iteration 0 of 82 with  training loss 0.03808651119470596.\n",
      "Iteration 16 of 82 with  training loss 0.036670781672000885.\n",
      "Iteration 32 of 82 with  training loss 0.04173778370022774.\n",
      "Iteration 48 of 82 with  training loss 0.04258662462234497.\n",
      "Iteration 64 of 82 with  training loss 0.041747964918613434.\n",
      "Iteration 80 of 82 with  training loss 0.04322635382413864.\n",
      "Iteration 0 of 83 with  training loss 0.03724074736237526.\n",
      "Iteration 16 of 83 with  training loss 0.03526853024959564.\n",
      "Iteration 32 of 83 with  training loss 0.04084298387169838.\n",
      "Iteration 48 of 83 with  training loss 0.04243490844964981.\n",
      "Iteration 64 of 83 with  training loss 0.04165766015648842.\n",
      "Iteration 80 of 83 with  training loss 0.04222909361124039.\n",
      "Iteration 0 of 84 with  training loss 0.03850981220602989.\n",
      "Iteration 16 of 84 with  training loss 0.04409570246934891.\n",
      "Iteration 32 of 84 with  training loss 0.0368867963552475.\n",
      "Iteration 48 of 84 with  training loss 0.040928930044174194.\n",
      "Iteration 64 of 84 with  training loss 0.040884628891944885.\n",
      "Iteration 80 of 84 with  training loss 0.04193476215004921.\n",
      "Iteration 0 of 85 with  training loss 0.04393835365772247.\n",
      "Iteration 16 of 85 with  training loss 0.03908141702413559.\n",
      "Iteration 32 of 85 with  training loss 0.044052958488464355.\n",
      "Iteration 48 of 85 with  training loss 0.03982018679380417.\n",
      "Iteration 64 of 85 with  training loss 0.04448974132537842.\n",
      "Iteration 80 of 85 with  training loss 0.04004780203104019.\n",
      "Iteration 0 of 86 with  training loss 0.043868549168109894.\n",
      "Iteration 16 of 86 with  training loss 0.04057613015174866.\n",
      "Iteration 32 of 86 with  training loss 0.03813176602125168.\n",
      "Iteration 48 of 86 with  training loss 0.04351724684238434.\n",
      "Iteration 64 of 86 with  training loss 0.04027658700942993.\n",
      "Iteration 80 of 86 with  training loss 0.04049064591526985.\n",
      "Iteration 0 of 87 with  training loss 0.042309027165174484.\n",
      "Iteration 16 of 87 with  training loss 0.044855162501335144.\n",
      "Iteration 32 of 87 with  training loss 0.042483851313591.\n",
      "Iteration 48 of 87 with  training loss 0.04456997290253639.\n",
      "Iteration 64 of 87 with  training loss 0.039620038121938705.\n",
      "Iteration 80 of 87 with  training loss 0.0404549166560173.\n",
      "Iteration 0 of 88 with  training loss 0.03982093185186386.\n",
      "Iteration 16 of 88 with  training loss 0.041570596396923065.\n",
      "Iteration 32 of 88 with  training loss 0.04250914603471756.\n",
      "Iteration 48 of 88 with  training loss 0.040315695106983185.\n",
      "Iteration 64 of 88 with  training loss 0.04357194900512695.\n",
      "Iteration 80 of 88 with  training loss 0.0385836698114872.\n",
      "Iteration 0 of 89 with  training loss 0.04131203517317772.\n",
      "Iteration 16 of 89 with  training loss 0.037525687366724014.\n",
      "Iteration 32 of 89 with  training loss 0.03727996349334717.\n",
      "Iteration 48 of 89 with  training loss 0.04191543161869049.\n",
      "Iteration 64 of 89 with  training loss 0.04205354303121567.\n",
      "Iteration 80 of 89 with  training loss 0.04049959033727646.\n",
      "Iteration 0 of 90 with  training loss 0.04163132607936859.\n",
      "Iteration 16 of 90 with  training loss 0.04031457379460335.\n",
      "Iteration 32 of 90 with  training loss 0.04060040041804314.\n",
      "Iteration 48 of 90 with  training loss 0.03397735208272934.\n",
      "Iteration 64 of 90 with  training loss 0.04249177500605583.\n",
      "Iteration 80 of 90 with  training loss 0.0400695726275444.\n",
      "Iteration 0 of 91 with  training loss 0.036867767572402954.\n",
      "Iteration 16 of 91 with  training loss 0.03447675332427025.\n",
      "Iteration 32 of 91 with  training loss 0.039397142827510834.\n",
      "Iteration 48 of 91 with  training loss 0.04120771586894989.\n",
      "Iteration 64 of 91 with  training loss 0.03961770981550217.\n",
      "Iteration 80 of 91 with  training loss 0.03918026387691498.\n",
      "Iteration 0 of 92 with  training loss 0.04032278060913086.\n",
      "Iteration 16 of 92 with  training loss 0.040118902921676636.\n",
      "Iteration 32 of 92 with  training loss 0.03846385329961777.\n",
      "Iteration 48 of 92 with  training loss 0.04253323748707771.\n",
      "Iteration 64 of 92 with  training loss 0.04116750508546829.\n",
      "Iteration 80 of 92 with  training loss 0.03986755758523941.\n",
      "Iteration 0 of 93 with  training loss 0.04125460609793663.\n",
      "Iteration 16 of 93 with  training loss 0.04142220318317413.\n",
      "Iteration 32 of 93 with  training loss 0.036599162966012955.\n",
      "Iteration 48 of 93 with  training loss 0.044182997196912766.\n",
      "Iteration 64 of 93 with  training loss 0.04116853326559067.\n",
      "Iteration 80 of 93 with  training loss 0.04045671224594116.\n",
      "Iteration 0 of 94 with  training loss 0.04243108257651329.\n",
      "Iteration 16 of 94 with  training loss 0.03888725861907005.\n",
      "Iteration 32 of 94 with  training loss 0.041242167353630066.\n",
      "Iteration 48 of 94 with  training loss 0.03778861463069916.\n",
      "Iteration 64 of 94 with  training loss 0.04106411710381508.\n",
      "Iteration 80 of 94 with  training loss 0.04026304930448532.\n",
      "Iteration 0 of 95 with  training loss 0.03943388909101486.\n",
      "Iteration 16 of 95 with  training loss 0.03949002921581268.\n",
      "Iteration 32 of 95 with  training loss 0.03853524476289749.\n",
      "Iteration 48 of 95 with  training loss 0.04176022857427597.\n",
      "Iteration 64 of 95 with  training loss 0.041291091591119766.\n",
      "Iteration 80 of 95 with  training loss 0.03831181675195694.\n",
      "Iteration 0 of 96 with  training loss 0.04079654812812805.\n",
      "Iteration 16 of 96 with  training loss 0.040410637855529785.\n",
      "Iteration 32 of 96 with  training loss 0.037322595715522766.\n",
      "Iteration 48 of 96 with  training loss 0.03677070885896683.\n",
      "Iteration 64 of 96 with  training loss 0.04149322211742401.\n",
      "Iteration 80 of 96 with  training loss 0.03884435072541237.\n",
      "Iteration 0 of 97 with  training loss 0.04214249923825264.\n",
      "Iteration 16 of 97 with  training loss 0.03814452886581421.\n",
      "Iteration 32 of 97 with  training loss 0.039104215800762177.\n",
      "Iteration 48 of 97 with  training loss 0.04089701920747757.\n",
      "Iteration 64 of 97 with  training loss 0.04210228472948074.\n",
      "Iteration 80 of 97 with  training loss 0.04203539341688156.\n",
      "Iteration 0 of 98 with  training loss 0.038811713457107544.\n",
      "Iteration 16 of 98 with  training loss 0.03915192186832428.\n",
      "Iteration 32 of 98 with  training loss 0.03597581014037132.\n",
      "Iteration 48 of 98 with  training loss 0.03725579380989075.\n",
      "Iteration 64 of 98 with  training loss 0.04187607765197754.\n",
      "Iteration 80 of 98 with  training loss 0.036216579377651215.\n",
      "Iteration 0 of 99 with  training loss 0.036714985966682434.\n",
      "Iteration 16 of 99 with  training loss 0.041955869644880295.\n",
      "Iteration 32 of 99 with  training loss 0.039268918335437775.\n",
      "Iteration 48 of 99 with  training loss 0.04383764788508415.\n",
      "Iteration 64 of 99 with  training loss 0.03976508975028992.\n",
      "Iteration 80 of 99 with  training loss 0.03776521608233452.\n",
      "Iteration 0 of 100 with  training loss 0.03988625109195709.\n",
      "Iteration 16 of 100 with  training loss 0.037850696593523026.\n",
      "Iteration 32 of 100 with  training loss 0.043399594724178314.\n",
      "Iteration 48 of 100 with  training loss 0.04056299105286598.\n",
      "Iteration 64 of 100 with  training loss 0.043678443878889084.\n",
      "Iteration 80 of 100 with  training loss 0.04033432900905609.\n",
      "Iteration 0 of 101 with  training loss 0.04073028266429901.\n",
      "Iteration 16 of 101 with  training loss 0.04012187942862511.\n",
      "Iteration 32 of 101 with  training loss 0.03806041553616524.\n",
      "Iteration 48 of 101 with  training loss 0.039875298738479614.\n",
      "Iteration 64 of 101 with  training loss 0.040083929896354675.\n",
      "Iteration 80 of 101 with  training loss 0.04314953833818436.\n",
      "Iteration 0 of 102 with  training loss 0.04071597754955292.\n",
      "Iteration 16 of 102 with  training loss 0.03994195535778999.\n",
      "Iteration 32 of 102 with  training loss 0.03931433707475662.\n",
      "Iteration 48 of 102 with  training loss 0.04066328704357147.\n",
      "Iteration 64 of 102 with  training loss 0.04164574667811394.\n",
      "Iteration 80 of 102 with  training loss 0.0418006107211113.\n",
      "Iteration 0 of 103 with  training loss 0.03840241953730583.\n",
      "Iteration 16 of 103 with  training loss 0.041666869074106216.\n",
      "Iteration 32 of 103 with  training loss 0.03954055905342102.\n",
      "Iteration 48 of 103 with  training loss 0.03986465185880661.\n",
      "Iteration 64 of 103 with  training loss 0.03921148180961609.\n",
      "Iteration 80 of 103 with  training loss 0.0423102080821991.\n",
      "Iteration 0 of 104 with  training loss 0.0394345186650753.\n",
      "Iteration 16 of 104 with  training loss 0.04010837897658348.\n",
      "Iteration 32 of 104 with  training loss 0.042406342923641205.\n",
      "Iteration 48 of 104 with  training loss 0.04322570562362671.\n",
      "Iteration 64 of 104 with  training loss 0.03818224370479584.\n",
      "Iteration 80 of 104 with  training loss 0.03949158638715744.\n",
      "Iteration 0 of 105 with  training loss 0.03993501514196396.\n",
      "Iteration 16 of 105 with  training loss 0.037720806896686554.\n",
      "Iteration 32 of 105 with  training loss 0.03748277574777603.\n",
      "Iteration 48 of 105 with  training loss 0.03388766199350357.\n",
      "Iteration 64 of 105 with  training loss 0.03856833651661873.\n",
      "Iteration 80 of 105 with  training loss 0.039458997547626495.\n",
      "Iteration 0 of 106 with  training loss 0.04271479696035385.\n",
      "Iteration 16 of 106 with  training loss 0.03637899458408356.\n",
      "Iteration 32 of 106 with  training loss 0.03796391189098358.\n",
      "Iteration 48 of 106 with  training loss 0.03714051842689514.\n",
      "Iteration 64 of 106 with  training loss 0.03599026799201965.\n",
      "Iteration 80 of 106 with  training loss 0.03742644563317299.\n",
      "Iteration 0 of 107 with  training loss 0.04003926366567612.\n",
      "Iteration 16 of 107 with  training loss 0.03588741645216942.\n",
      "Iteration 32 of 107 with  training loss 0.03718382865190506.\n",
      "Iteration 48 of 107 with  training loss 0.04141826927661896.\n",
      "Iteration 64 of 107 with  training loss 0.04099655896425247.\n",
      "Iteration 80 of 107 with  training loss 0.040186911821365356.\n",
      "Iteration 0 of 108 with  training loss 0.04197169840335846.\n",
      "Iteration 16 of 108 with  training loss 0.04024454206228256.\n",
      "Iteration 32 of 108 with  training loss 0.044008366763591766.\n",
      "Iteration 48 of 108 with  training loss 0.042940158396959305.\n",
      "Iteration 64 of 108 with  training loss 0.043139051645994186.\n",
      "Iteration 80 of 108 with  training loss 0.04148145392537117.\n",
      "Iteration 0 of 109 with  training loss 0.036281272768974304.\n",
      "Iteration 16 of 109 with  training loss 0.03997509181499481.\n",
      "Iteration 32 of 109 with  training loss 0.03926217555999756.\n",
      "Iteration 48 of 109 with  training loss 0.04090593755245209.\n",
      "Iteration 64 of 109 with  training loss 0.042181894183158875.\n",
      "Iteration 80 of 109 with  training loss 0.04029553756117821.\n",
      "Iteration 0 of 110 with  training loss 0.03606557846069336.\n",
      "Iteration 16 of 110 with  training loss 0.04154723882675171.\n",
      "Iteration 32 of 110 with  training loss 0.04194589704275131.\n",
      "Iteration 48 of 110 with  training loss 0.03795609250664711.\n",
      "Iteration 64 of 110 with  training loss 0.040488358587026596.\n",
      "Iteration 80 of 110 with  training loss 0.03884640336036682.\n",
      "Iteration 0 of 111 with  training loss 0.040542107075452805.\n",
      "Iteration 16 of 111 with  training loss 0.04292573407292366.\n",
      "Iteration 32 of 111 with  training loss 0.03948134928941727.\n",
      "Iteration 48 of 111 with  training loss 0.04115866869688034.\n",
      "Iteration 64 of 111 with  training loss 0.03706846386194229.\n",
      "Iteration 80 of 111 with  training loss 0.03755584731698036.\n",
      "Iteration 0 of 112 with  training loss 0.04230939596891403.\n",
      "Iteration 16 of 112 with  training loss 0.03911495581269264.\n",
      "Iteration 32 of 112 with  training loss 0.04253572225570679.\n",
      "Iteration 48 of 112 with  training loss 0.03987099975347519.\n",
      "Iteration 64 of 112 with  training loss 0.039887163788080215.\n",
      "Iteration 80 of 112 with  training loss 0.03919694200158119.\n",
      "Iteration 0 of 113 with  training loss 0.033815693110227585.\n",
      "Iteration 16 of 113 with  training loss 0.041271913796663284.\n",
      "Iteration 32 of 113 with  training loss 0.03773089125752449.\n",
      "Iteration 48 of 113 with  training loss 0.03710591793060303.\n",
      "Iteration 64 of 113 with  training loss 0.03990817815065384.\n",
      "Iteration 80 of 113 with  training loss 0.03911183401942253.\n",
      "Iteration 0 of 114 with  training loss 0.04239337891340256.\n",
      "Iteration 16 of 114 with  training loss 0.036665841937065125.\n",
      "Iteration 32 of 114 with  training loss 0.03912772238254547.\n",
      "Iteration 48 of 114 with  training loss 0.03756747394800186.\n",
      "Iteration 64 of 114 with  training loss 0.04158153384923935.\n",
      "Iteration 80 of 114 with  training loss 0.037601567804813385.\n",
      "Iteration 0 of 115 with  training loss 0.036309488117694855.\n",
      "Iteration 16 of 115 with  training loss 0.04139430820941925.\n",
      "Iteration 32 of 115 with  training loss 0.03919380158185959.\n",
      "Iteration 48 of 115 with  training loss 0.041848570108413696.\n",
      "Iteration 64 of 115 with  training loss 0.03810429573059082.\n",
      "Iteration 80 of 115 with  training loss 0.034042250365018845.\n",
      "Iteration 0 of 116 with  training loss 0.038126301020383835.\n",
      "Iteration 16 of 116 with  training loss 0.03740478307008743.\n",
      "Iteration 32 of 116 with  training loss 0.03984449803829193.\n",
      "Iteration 48 of 116 with  training loss 0.036312539130449295.\n",
      "Iteration 64 of 116 with  training loss 0.04171041399240494.\n",
      "Iteration 80 of 116 with  training loss 0.038871489465236664.\n",
      "Iteration 0 of 117 with  training loss 0.04236200079321861.\n",
      "Iteration 16 of 117 with  training loss 0.039679668843746185.\n",
      "Iteration 32 of 117 with  training loss 0.03791278600692749.\n",
      "Iteration 48 of 117 with  training loss 0.04306410253047943.\n",
      "Iteration 64 of 117 with  training loss 0.03829648345708847.\n",
      "Iteration 80 of 117 with  training loss 0.03718775510787964.\n",
      "Iteration 0 of 118 with  training loss 0.041576240211725235.\n",
      "Iteration 16 of 118 with  training loss 0.03775012493133545.\n",
      "Iteration 32 of 118 with  training loss 0.040008604526519775.\n",
      "Iteration 48 of 118 with  training loss 0.04094438999891281.\n",
      "Iteration 64 of 118 with  training loss 0.037847571074962616.\n",
      "Iteration 80 of 118 with  training loss 0.03673757612705231.\n",
      "Iteration 0 of 119 with  training loss 0.04253371059894562.\n",
      "Iteration 16 of 119 with  training loss 0.044627703726291656.\n",
      "Iteration 32 of 119 with  training loss 0.03721275180578232.\n",
      "Iteration 48 of 119 with  training loss 0.036042287945747375.\n",
      "Iteration 64 of 119 with  training loss 0.04358302801847458.\n",
      "Iteration 80 of 119 with  training loss 0.04044728726148605.\n",
      "Iteration 0 of 120 with  training loss 0.03819354251027107.\n",
      "Iteration 16 of 120 with  training loss 0.03665025532245636.\n",
      "Iteration 32 of 120 with  training loss 0.039650335907936096.\n",
      "Iteration 48 of 120 with  training loss 0.04053575173020363.\n",
      "Iteration 64 of 120 with  training loss 0.03869889676570892.\n",
      "Iteration 80 of 120 with  training loss 0.038245949894189835.\n",
      "Iteration 0 of 121 with  training loss 0.03784655034542084.\n",
      "Iteration 16 of 121 with  training loss 0.03331274539232254.\n",
      "Iteration 32 of 121 with  training loss 0.0343378409743309.\n",
      "Iteration 48 of 121 with  training loss 0.037648603320121765.\n",
      "Iteration 64 of 121 with  training loss 0.038865745067596436.\n",
      "Iteration 80 of 121 with  training loss 0.04028933122754097.\n",
      "Iteration 0 of 122 with  training loss 0.04035986214876175.\n",
      "Iteration 16 of 122 with  training loss 0.039710573852062225.\n",
      "Iteration 32 of 122 with  training loss 0.03917641192674637.\n",
      "Iteration 48 of 122 with  training loss 0.04250199347734451.\n",
      "Iteration 64 of 122 with  training loss 0.03548301011323929.\n",
      "Iteration 80 of 122 with  training loss 0.03522452712059021.\n",
      "Iteration 0 of 123 with  training loss 0.038725242018699646.\n",
      "Iteration 16 of 123 with  training loss 0.04235903173685074.\n",
      "Iteration 32 of 123 with  training loss 0.03882655128836632.\n",
      "Iteration 48 of 123 with  training loss 0.04232768714427948.\n",
      "Iteration 64 of 123 with  training loss 0.040967315435409546.\n",
      "Iteration 80 of 123 with  training loss 0.044634122401475906.\n",
      "Iteration 0 of 124 with  training loss 0.03602522611618042.\n",
      "Iteration 16 of 124 with  training loss 0.037604980170726776.\n",
      "Iteration 32 of 124 with  training loss 0.04209282249212265.\n",
      "Iteration 48 of 124 with  training loss 0.04203313961625099.\n",
      "Iteration 64 of 124 with  training loss 0.03993099555373192.\n",
      "Iteration 80 of 124 with  training loss 0.0410928949713707.\n",
      "Iteration 0 of 125 with  training loss 0.036979563534259796.\n",
      "Iteration 16 of 125 with  training loss 0.03645635396242142.\n",
      "Iteration 32 of 125 with  training loss 0.03894118219614029.\n",
      "Iteration 48 of 125 with  training loss 0.03992573916912079.\n",
      "Iteration 64 of 125 with  training loss 0.04022933915257454.\n",
      "Iteration 80 of 125 with  training loss 0.041261449456214905.\n",
      "Iteration 0 of 126 with  training loss 0.04178418591618538.\n",
      "Iteration 16 of 126 with  training loss 0.04030459374189377.\n",
      "Iteration 32 of 126 with  training loss 0.039375558495521545.\n",
      "Iteration 48 of 126 with  training loss 0.035925693809986115.\n",
      "Iteration 64 of 126 with  training loss 0.037787750363349915.\n",
      "Iteration 80 of 126 with  training loss 0.041735753417015076.\n",
      "Iteration 0 of 127 with  training loss 0.04132114350795746.\n",
      "Iteration 16 of 127 with  training loss 0.03537029027938843.\n",
      "Iteration 32 of 127 with  training loss 0.038660041987895966.\n",
      "Iteration 48 of 127 with  training loss 0.03775621950626373.\n",
      "Iteration 64 of 127 with  training loss 0.03802230954170227.\n",
      "Iteration 80 of 127 with  training loss 0.03446746617555618.\n",
      "Iteration 0 of 128 with  training loss 0.038899295032024384.\n",
      "Iteration 16 of 128 with  training loss 0.039492420852184296.\n",
      "Iteration 32 of 128 with  training loss 0.03416987508535385.\n",
      "Iteration 48 of 128 with  training loss 0.037356533110141754.\n",
      "Iteration 64 of 128 with  training loss 0.036484118551015854.\n",
      "Iteration 80 of 128 with  training loss 0.04146037995815277.\n",
      "Iteration 0 of 129 with  training loss 0.03770775347948074.\n",
      "Iteration 16 of 129 with  training loss 0.03767014294862747.\n",
      "Iteration 32 of 129 with  training loss 0.03459051251411438.\n",
      "Iteration 48 of 129 with  training loss 0.0402757003903389.\n",
      "Iteration 64 of 129 with  training loss 0.03847295790910721.\n",
      "Iteration 80 of 129 with  training loss 0.03566053509712219.\n",
      "Iteration 0 of 130 with  training loss 0.04279821738600731.\n",
      "Iteration 16 of 130 with  training loss 0.036571308970451355.\n",
      "Iteration 32 of 130 with  training loss 0.039016108959913254.\n",
      "Iteration 48 of 130 with  training loss 0.04078606516122818.\n",
      "Iteration 64 of 130 with  training loss 0.03716898709535599.\n",
      "Iteration 80 of 130 with  training loss 0.036024510860443115.\n",
      "Iteration 0 of 131 with  training loss 0.03841669484972954.\n",
      "Iteration 16 of 131 with  training loss 0.03701408952474594.\n",
      "Iteration 32 of 131 with  training loss 0.03539364039897919.\n",
      "Iteration 48 of 131 with  training loss 0.03790045529603958.\n",
      "Iteration 64 of 131 with  training loss 0.03889787197113037.\n",
      "Iteration 80 of 131 with  training loss 0.0393252819776535.\n",
      "Iteration 0 of 132 with  training loss 0.03653942793607712.\n",
      "Iteration 16 of 132 with  training loss 0.036692000925540924.\n",
      "Iteration 32 of 132 with  training loss 0.039550043642520905.\n",
      "Iteration 48 of 132 with  training loss 0.03679783642292023.\n",
      "Iteration 64 of 132 with  training loss 0.042030252516269684.\n",
      "Iteration 80 of 132 with  training loss 0.03811347484588623.\n",
      "Iteration 0 of 133 with  training loss 0.03520279377698898.\n",
      "Iteration 16 of 133 with  training loss 0.0370129719376564.\n",
      "Iteration 32 of 133 with  training loss 0.037341490387916565.\n",
      "Iteration 48 of 133 with  training loss 0.03717343881726265.\n",
      "Iteration 64 of 133 with  training loss 0.03800756484270096.\n",
      "Iteration 80 of 133 with  training loss 0.040800243616104126.\n",
      "Iteration 0 of 134 with  training loss 0.03956369683146477.\n",
      "Iteration 16 of 134 with  training loss 0.04046051949262619.\n",
      "Iteration 32 of 134 with  training loss 0.03525181859731674.\n",
      "Iteration 48 of 134 with  training loss 0.03710451349616051.\n",
      "Iteration 64 of 134 with  training loss 0.03911066800355911.\n",
      "Iteration 80 of 134 with  training loss 0.03890937566757202.\n",
      "Iteration 0 of 135 with  training loss 0.039643384516239166.\n",
      "Iteration 16 of 135 with  training loss 0.03461993858218193.\n",
      "Iteration 32 of 135 with  training loss 0.036642611026763916.\n",
      "Iteration 48 of 135 with  training loss 0.03721965104341507.\n",
      "Iteration 64 of 135 with  training loss 0.03811655566096306.\n",
      "Iteration 80 of 135 with  training loss 0.036806389689445496.\n",
      "Iteration 0 of 136 with  training loss 0.038106538355350494.\n",
      "Iteration 16 of 136 with  training loss 0.03743888810276985.\n",
      "Iteration 32 of 136 with  training loss 0.037998490035533905.\n",
      "Iteration 48 of 136 with  training loss 0.03966538608074188.\n",
      "Iteration 64 of 136 with  training loss 0.04095248132944107.\n",
      "Iteration 80 of 136 with  training loss 0.03984733670949936.\n",
      "Iteration 0 of 137 with  training loss 0.039566513150930405.\n",
      "Iteration 16 of 137 with  training loss 0.041307106614112854.\n",
      "Iteration 32 of 137 with  training loss 0.04003746807575226.\n",
      "Iteration 48 of 137 with  training loss 0.03701313957571983.\n",
      "Iteration 64 of 137 with  training loss 0.03698258846998215.\n",
      "Iteration 80 of 137 with  training loss 0.03702957555651665.\n",
      "Iteration 0 of 138 with  training loss 0.03676266968250275.\n",
      "Iteration 16 of 138 with  training loss 0.03455575555562973.\n",
      "Iteration 32 of 138 with  training loss 0.035694804042577744.\n",
      "Iteration 48 of 138 with  training loss 0.03476481884717941.\n",
      "Iteration 64 of 138 with  training loss 0.03663449361920357.\n",
      "Iteration 80 of 138 with  training loss 0.03889743983745575.\n",
      "Iteration 0 of 139 with  training loss 0.038411881774663925.\n",
      "Iteration 16 of 139 with  training loss 0.035364195704460144.\n",
      "Iteration 32 of 139 with  training loss 0.039948638528585434.\n",
      "Iteration 48 of 139 with  training loss 0.03900187835097313.\n",
      "Iteration 64 of 139 with  training loss 0.03913557529449463.\n",
      "Iteration 80 of 139 with  training loss 0.038639165461063385.\n",
      "Iteration 0 of 140 with  training loss 0.04126359522342682.\n",
      "Iteration 16 of 140 with  training loss 0.03408607840538025.\n",
      "Iteration 32 of 140 with  training loss 0.040143489837646484.\n",
      "Iteration 48 of 140 with  training loss 0.03533434122800827.\n",
      "Iteration 64 of 140 with  training loss 0.03745603561401367.\n",
      "Iteration 80 of 140 with  training loss 0.03751376271247864.\n",
      "Iteration 0 of 141 with  training loss 0.03732039034366608.\n",
      "Iteration 16 of 141 with  training loss 0.037508461624383926.\n",
      "Iteration 32 of 141 with  training loss 0.03796529769897461.\n",
      "Iteration 48 of 141 with  training loss 0.03389361500740051.\n",
      "Iteration 64 of 141 with  training loss 0.035885557532310486.\n",
      "Iteration 80 of 141 with  training loss 0.03515786677598953.\n",
      "Iteration 0 of 142 with  training loss 0.03791215270757675.\n",
      "Iteration 16 of 142 with  training loss 0.042023979127407074.\n",
      "Iteration 32 of 142 with  training loss 0.03798891231417656.\n",
      "Iteration 48 of 142 with  training loss 0.0396769642829895.\n",
      "Iteration 64 of 142 with  training loss 0.040977250784635544.\n",
      "Iteration 80 of 142 with  training loss 0.03722884878516197.\n",
      "Iteration 0 of 143 with  training loss 0.0339384451508522.\n",
      "Iteration 16 of 143 with  training loss 0.0354011096060276.\n",
      "Iteration 32 of 143 with  training loss 0.034450702369213104.\n",
      "Iteration 48 of 143 with  training loss 0.03942563012242317.\n",
      "Iteration 64 of 143 with  training loss 0.03853125870227814.\n",
      "Iteration 80 of 143 with  training loss 0.03676186874508858.\n",
      "Iteration 0 of 144 with  training loss 0.042480699717998505.\n",
      "Iteration 16 of 144 with  training loss 0.0397888645529747.\n",
      "Iteration 32 of 144 with  training loss 0.04012584313750267.\n",
      "Iteration 48 of 144 with  training loss 0.038026727735996246.\n",
      "Iteration 64 of 144 with  training loss 0.03506947308778763.\n",
      "Iteration 80 of 144 with  training loss 0.03795637935400009.\n",
      "Iteration 0 of 145 with  training loss 0.03882743418216705.\n",
      "Iteration 16 of 145 with  training loss 0.03898011893033981.\n",
      "Iteration 32 of 145 with  training loss 0.03749321773648262.\n",
      "Iteration 48 of 145 with  training loss 0.03860299289226532.\n",
      "Iteration 64 of 145 with  training loss 0.03475067764520645.\n",
      "Iteration 80 of 145 with  training loss 0.03500191867351532.\n",
      "Iteration 0 of 146 with  training loss 0.030812954530119896.\n",
      "Iteration 16 of 146 with  training loss 0.040354859083890915.\n",
      "Iteration 32 of 146 with  training loss 0.03697987645864487.\n",
      "Iteration 48 of 146 with  training loss 0.04022867977619171.\n",
      "Iteration 64 of 146 with  training loss 0.036892443895339966.\n",
      "Iteration 80 of 146 with  training loss 0.03323863819241524.\n",
      "Iteration 0 of 147 with  training loss 0.03636474907398224.\n",
      "Iteration 16 of 147 with  training loss 0.037295714020729065.\n",
      "Iteration 32 of 147 with  training loss 0.043946728110313416.\n",
      "Iteration 48 of 147 with  training loss 0.03696936368942261.\n",
      "Iteration 64 of 147 with  training loss 0.03501532971858978.\n",
      "Iteration 80 of 147 with  training loss 0.0409059077501297.\n",
      "Iteration 0 of 148 with  training loss 0.03864183649420738.\n",
      "Iteration 16 of 148 with  training loss 0.03864810988306999.\n",
      "Iteration 32 of 148 with  training loss 0.037701427936553955.\n",
      "Iteration 48 of 148 with  training loss 0.038442812860012054.\n",
      "Iteration 64 of 148 with  training loss 0.038049355149269104.\n",
      "Iteration 80 of 148 with  training loss 0.03864765167236328.\n",
      "Iteration 0 of 149 with  training loss 0.03695997595787048.\n",
      "Iteration 16 of 149 with  training loss 0.03963740915060043.\n",
      "Iteration 32 of 149 with  training loss 0.03722955659031868.\n",
      "Iteration 48 of 149 with  training loss 0.0366051010787487.\n",
      "Iteration 64 of 149 with  training loss 0.03684423118829727.\n",
      "Iteration 80 of 149 with  training loss 0.0350954607129097.\n",
      "Iteration 0 of 150 with  training loss 0.036279015243053436.\n",
      "Iteration 16 of 150 with  training loss 0.03686133027076721.\n",
      "Iteration 32 of 150 with  training loss 0.04141341894865036.\n",
      "Iteration 48 of 150 with  training loss 0.03628450632095337.\n",
      "Iteration 64 of 150 with  training loss 0.03591479733586311.\n",
      "Iteration 80 of 150 with  training loss 0.03712160885334015.\n",
      "Iteration 0 of 151 with  training loss 0.03765926510095596.\n",
      "Iteration 16 of 151 with  training loss 0.03949463367462158.\n",
      "Iteration 32 of 151 with  training loss 0.04082370921969414.\n",
      "Iteration 48 of 151 with  training loss 0.036693572998046875.\n",
      "Iteration 64 of 151 with  training loss 0.03682691976428032.\n",
      "Iteration 80 of 151 with  training loss 0.036549896001815796.\n",
      "Iteration 0 of 152 with  training loss 0.03895754739642143.\n",
      "Iteration 16 of 152 with  training loss 0.03783649951219559.\n",
      "Iteration 32 of 152 with  training loss 0.03613634407520294.\n",
      "Iteration 48 of 152 with  training loss 0.03944120556116104.\n",
      "Iteration 64 of 152 with  training loss 0.035751864314079285.\n",
      "Iteration 80 of 152 with  training loss 0.04079117998480797.\n",
      "Iteration 0 of 153 with  training loss 0.03503378480672836.\n",
      "Iteration 16 of 153 with  training loss 0.037840619683265686.\n",
      "Iteration 32 of 153 with  training loss 0.03756982088088989.\n",
      "Iteration 48 of 153 with  training loss 0.038586728274822235.\n",
      "Iteration 64 of 153 with  training loss 0.03888531029224396.\n",
      "Iteration 80 of 153 with  training loss 0.037510357797145844.\n",
      "Iteration 0 of 154 with  training loss 0.037706635892391205.\n",
      "Iteration 16 of 154 with  training loss 0.03670179098844528.\n",
      "Iteration 32 of 154 with  training loss 0.03570803627371788.\n",
      "Iteration 48 of 154 with  training loss 0.034847140312194824.\n",
      "Iteration 64 of 154 with  training loss 0.033556465059518814.\n",
      "Iteration 80 of 154 with  training loss 0.03412776440382004.\n",
      "Iteration 0 of 155 with  training loss 0.041414983570575714.\n",
      "Iteration 16 of 155 with  training loss 0.03721427917480469.\n",
      "Iteration 32 of 155 with  training loss 0.03987298905849457.\n",
      "Iteration 48 of 155 with  training loss 0.0395970493555069.\n",
      "Iteration 64 of 155 with  training loss 0.034025322645902634.\n",
      "Iteration 80 of 155 with  training loss 0.03698346018791199.\n",
      "Iteration 0 of 156 with  training loss 0.0407070517539978.\n",
      "Iteration 16 of 156 with  training loss 0.03844762593507767.\n",
      "Iteration 32 of 156 with  training loss 0.03333917260169983.\n",
      "Iteration 48 of 156 with  training loss 0.038321495056152344.\n",
      "Iteration 64 of 156 with  training loss 0.03598584234714508.\n",
      "Iteration 80 of 156 with  training loss 0.03454732149839401.\n",
      "Iteration 0 of 157 with  training loss 0.03884964436292648.\n",
      "Iteration 16 of 157 with  training loss 0.03962520882487297.\n",
      "Iteration 32 of 157 with  training loss 0.03676192834973335.\n",
      "Iteration 48 of 157 with  training loss 0.034178197383880615.\n",
      "Iteration 64 of 157 with  training loss 0.039414260536432266.\n",
      "Iteration 80 of 157 with  training loss 0.03588474169373512.\n",
      "Iteration 0 of 158 with  training loss 0.038077473640441895.\n",
      "Iteration 16 of 158 with  training loss 0.037851497530937195.\n",
      "Iteration 32 of 158 with  training loss 0.03835459053516388.\n",
      "Iteration 48 of 158 with  training loss 0.037570349872112274.\n",
      "Iteration 64 of 158 with  training loss 0.0394543781876564.\n",
      "Iteration 80 of 158 with  training loss 0.037208981812000275.\n",
      "Iteration 0 of 159 with  training loss 0.03493289276957512.\n",
      "Iteration 16 of 159 with  training loss 0.037282466888427734.\n",
      "Iteration 32 of 159 with  training loss 0.04103979468345642.\n",
      "Iteration 48 of 159 with  training loss 0.03556409105658531.\n",
      "Iteration 64 of 159 with  training loss 0.03818391263484955.\n",
      "Iteration 80 of 159 with  training loss 0.037467245012521744.\n",
      "Iteration 0 of 160 with  training loss 0.04025677591562271.\n",
      "Iteration 16 of 160 with  training loss 0.035384401679039.\n",
      "Iteration 32 of 160 with  training loss 0.03502112627029419.\n",
      "Iteration 48 of 160 with  training loss 0.03698080778121948.\n",
      "Iteration 64 of 160 with  training loss 0.03628508001565933.\n",
      "Iteration 80 of 160 with  training loss 0.03631854057312012.\n",
      "Iteration 0 of 161 with  training loss 0.03846040368080139.\n",
      "Iteration 16 of 161 with  training loss 0.03677338361740112.\n",
      "Iteration 32 of 161 with  training loss 0.04008413106203079.\n",
      "Iteration 48 of 161 with  training loss 0.035216763615608215.\n",
      "Iteration 64 of 161 with  training loss 0.043175678700208664.\n",
      "Iteration 80 of 161 with  training loss 0.03561277687549591.\n",
      "Iteration 0 of 162 with  training loss 0.03248852491378784.\n",
      "Iteration 16 of 162 with  training loss 0.03860708698630333.\n",
      "Iteration 32 of 162 with  training loss 0.03293544426560402.\n",
      "Iteration 48 of 162 with  training loss 0.041977185755968094.\n",
      "Iteration 64 of 162 with  training loss 0.04010564088821411.\n",
      "Iteration 80 of 162 with  training loss 0.0353294238448143.\n",
      "Iteration 0 of 163 with  training loss 0.03709075599908829.\n",
      "Iteration 16 of 163 with  training loss 0.03863411396741867.\n",
      "Iteration 32 of 163 with  training loss 0.03502309322357178.\n",
      "Iteration 48 of 163 with  training loss 0.03431908041238785.\n",
      "Iteration 64 of 163 with  training loss 0.03497769683599472.\n",
      "Iteration 80 of 163 with  training loss 0.04151904582977295.\n",
      "Iteration 0 of 164 with  training loss 0.040703851729631424.\n",
      "Iteration 16 of 164 with  training loss 0.033578019589185715.\n",
      "Iteration 32 of 164 with  training loss 0.03519505262374878.\n",
      "Iteration 48 of 164 with  training loss 0.03704017028212547.\n",
      "Iteration 64 of 164 with  training loss 0.033090412616729736.\n",
      "Iteration 80 of 164 with  training loss 0.04183406010270119.\n",
      "Iteration 0 of 165 with  training loss 0.040156830102205276.\n",
      "Iteration 16 of 165 with  training loss 0.032775118947029114.\n",
      "Iteration 32 of 165 with  training loss 0.03786088153719902.\n",
      "Iteration 48 of 165 with  training loss 0.037077758461236954.\n",
      "Iteration 64 of 165 with  training loss 0.03734976798295975.\n",
      "Iteration 80 of 165 with  training loss 0.04105871915817261.\n",
      "Iteration 0 of 166 with  training loss 0.04050899296998978.\n",
      "Iteration 16 of 166 with  training loss 0.0368044450879097.\n",
      "Iteration 32 of 166 with  training loss 0.03667005896568298.\n",
      "Iteration 48 of 166 with  training loss 0.040222495794296265.\n",
      "Iteration 64 of 166 with  training loss 0.03518027439713478.\n",
      "Iteration 80 of 166 with  training loss 0.0379813015460968.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"gencast\",\n",
    "    # Track hyperparameters and run metadata\n",
    ")\n",
    "\n",
    "from graph_weather.models.gencast import Denoiser, WeightedMSELoss\n",
    "print(\"Initializing model\")\n",
    "device=\"cuda\"\n",
    "denoiser = Denoiser(\n",
    "    grid_lon=dataset.grid_lon,\n",
    "    grid_lat=dataset.grid_lat,\n",
    "    input_features_dim=dataset.input_features_dim,\n",
    "    output_features_dim=dataset.output_features_dim,\n",
    "    hidden_dims=[512, 512],\n",
    "    num_blocks=16,\n",
    "    num_heads=4,\n",
    "    splits=4,\n",
    "    num_hops=8,\n",
    "    device=device,\n",
    "    sparse=True,\n",
    "    use_edges_features=False\n",
    ").to(device)\n",
    "\n",
    "criterion = WeightedMSELoss(\n",
    "    grid_lat=torch.tensor(dataset.grid_lat, device=device),\n",
    "    pressure_levels=torch.tensor(dataset.pressure_levels, device=device),\n",
    "    num_atmospheric_features=len(dataset.atmospheric_features),\n",
    "    single_features_weights=torch.tensor([1.0, 0.1, 0.1, 0.1, 0.1], device=device),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(denoiser.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100)\n",
    "\n",
    "def split_batch(data, batch_size):\n",
    "    corrupted_targets, prev_inputs, noise_levels, target_residuals = data\n",
    "    n_samples = corrupted_targets.shape[0]\n",
    "    datas = []\n",
    "    for i in range(n_samples//batch_size):\n",
    "        datas.append((corrupted_targets[i*batch_size:(i+1)*batch_size], \n",
    "                    prev_inputs[i*batch_size:(i+1)*batch_size], \n",
    "                    noise_levels[i*batch_size:(i+1)*batch_size],\n",
    "                    target_residuals[i*batch_size:(i+1)*batch_size]))\n",
    "    return datas\n",
    "\n",
    "#denoiser.load_state_dict(torch.load(\"model_30.pt\"))    \n",
    "print(\"Starting training\")\n",
    "for i, data in enumerate(dataloader):\n",
    "    torch.save(denoiser.state_dict(), f\"model_{i}.pt\")\n",
    "    k=16\n",
    "    datas = split_batch(data, batch_size=k)\n",
    "    for j, sample in enumerate(datas):\n",
    "        corrupted_targets, prev_inputs, noise_levels, target_residuals = sample\n",
    "        denoiser.zero_grad()\n",
    "        preds = denoiser(\n",
    "            corrupted_targets=torch.tensor(corrupted_targets, dtype=torch.float32, device=device),\n",
    "            prev_inputs=torch.tensor(prev_inputs, dtype=torch.float32, device=device),\n",
    "            noise_levels=torch.tensor(noise_levels, dtype=torch.float32, device=device),\n",
    "        )\n",
    "        loss = criterion(preds, torch.tensor(noise_levels, dtype=torch.float32, device=device), torch.tensor(target_residuals, dtype=torch.float32, device=device))\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        scheduler.step(i//3 + j / (3*batch_size//k))\n",
    "        print(f\"Iteration {j*k} of {i} with  training loss {float(loss)}.\")\n",
    "        wandb.log({\"loss\": float(loss)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e78749d-f6f5-49c8-b058-525796fe7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 10:36:31 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:A1:00.0 Off |                  Off |\n",
      "| 30%   31C    P2              71W / 300W |  46376MiB / 49140MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
